<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>“2022-9-17-日记”</title>
      <link href="/post/39921.html"/>
      <url>/post/39921.html</url>
      
        <content type="html"><![CDATA[<h1 id="日记"><a href="#日记" class="headerlink" title="日记"></a>日记</h1><p>开学到现在已经18天了，距离上次日记也过去了11天了。</p><p>今天又发生了一件不是很愉快的事情，特此记录。</p><h1 id="感情问题"><a href="#感情问题" class="headerlink" title="感情问题"></a>感情问题</h1><p>从7号到现在，跟李聊了特别多，这次我吸取了以前的教训，不自己跟自己较劲，不自己为难自己，有问题及时解释清楚，感觉比以往的感情经历要顺利许多。</p><p>今天学校解封了，意味着我们两个有机会出来了，下周吧，把握机会，出去玩一玩。</p><h1 id="面试研会"><a href="#面试研会" class="headerlink" title="面试研会"></a>面试研会</h1><p>前几天面试了研会，本来其实没觉得这件事情有什么难，但是因为一些因素最终没有面试通过，一是确实没有准备，因为觉得是走过场，随便面试一下就好了，就纯临场发挥面试了，虽然自我感觉良好，但是可能在别人眼里是纯纯小丑吧。二是实验室三个师兄师姐都在里面，本以为靠这层关系也不会被刷，没想到纯纯小丑了。这种事也是自我催眠，其实根本是一厢情愿的事。</p><p>这应该是吧，纯面试被拒绝的第一次，以前本科还从没有过，连保研的时候，面试也从来都是加分项，没想到今天第一次遇到这种事。</p><p>给自己的教训应该有三点吧：</p><ol><li>凡事预则立不预则废，从中考历史，高考语文到今天，生活一而再再而三地给我教育我的是：最自信的地方最容易出问题，因为自信离自负太近，自负意味着盲目，也就大概率会出事。</li><li>无论什么时候，什么事都不要一厢情愿地去指望或者相信别人，把自己的命运交给别人是世界上最最最最蠢的事。</li><li>人外有人，这是从入学以来我极其深刻的感受，相比以前身边的人，现在周围的人无疑都优秀太多，哪怕是同一个本科出来的人，今天也在这个研会面试上恨恨地给了我一个耳光。以前那种从多方面高于别人的优越感现在不仅消失殆尽，而且反而产生了一种自卑情绪。只能说稳住情绪，认清自己吧。</li></ol><h1 id="对未来的迷茫"><a href="#对未来的迷茫" class="headerlink" title="对未来的迷茫"></a>对未来的迷茫</h1><p>时至今日，我依然不确定以后是搞学术，还是就业，更不要说搞学术是留校读博还是读完硕士再申请？往哪个方向就业？自己该做什么准备？这些详细的问题了。</p><p>我到底该怎么办？</p>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-9-6-日记</title>
      <link href="/post/13886.html"/>
      <url>/post/13886.html</url>
      
        <content type="html"><![CDATA[<h1 id="小BUG修复"><a href="#小BUG修复" class="headerlink" title="小BUG修复"></a>小BUG修复</h1><p>修复了部分封面图出现404 not found的小bug</p><h1 id="日记"><a href="#日记" class="headerlink" title="日记"></a>日记</h1><p>开学至今，已经是第七天了，这其中发生了一些事情，记录在这里。</p><h2 id="课程拉胯"><a href="#课程拉胯" class="headerlink" title="课程拉胯"></a>课程拉胯</h2><p>选课结果特别差，想学的几门课都没中，剩下的课又非常难，唉，感觉相比本科生课程，研究生的课的难度上了好几个台阶，尤其是计网和程序语言设计原理这两个，既要大作业又要考试，绝望。</p><p>此外还需要：自然辩证，算法设计，软件体系结构，矩阵这四门课在下学期修。</p><h2 id="实验室混子"><a href="#实验室混子" class="headerlink" title="实验室混子"></a>实验室混子</h2><p>实验室里好像只有我是个混子，无所事事。</p><p>其中研二的师兄师姐忙着写小论文，研三的师兄师姐忙着找工作，这都可以理解，研一的，似乎除了我之外，还有三个人，一个是本科就在实验室干活的，这个自然没法跟人家比较，他已经开始独当一面的设计项目了。剩下我们三个似乎也都没有什么方向感，无所事事居多，另外两个人有一个甚至实验室都没来过。相比其他人已经开始陷入极度的忙碌，这种混天过日子的生活让我产生了极大的焦虑。</p><p>而又有很多迷茫随之而来，要不要读博，看着实验室师兄师姐似乎找工作并不轻松，如果找工作最后的结果不仅如此人意呢？我是不是要跟师兄师姐那样，为了一份差强人意的工作，天天焦虑忙碌，而我能又不能接受这样庸碌平庸的去当一个打工人？硕转博么？我是不是愿意在北航读完博士？这个老师是不是适合我跟着他？我是不是应该读完硕士之后申请更好的学校的博士？按照目前这样的状况，我是不是只能混个毕业？我能不能做出支撑我申请博士的成果？没有成果，我又能不能通过考试上岸？</p><p>这些问题像是无数根拴在我心上的线，一想起来就拽的我浑身不自在。</p><p>在今年结束之前，我需要做出后续的详细计划。</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>寝室里现在三个人，一个大佬，天天七点起床去实验室，中午回来睡午觉，晚上忙完了回来跑步。说话也还算客气，就是这个天天起来这个闹钟非得放床下，真的吵。另一个摆烂，天天寝室里泡着，但是有ACM底子，也不愁找工作，难受的是也是手机放下面，从来不拿上去。凑合着过吧。</p><p>倒是感情方面有起色，我承认我也是真香定律的受害者，这几天天天跟人家聊到十二点，但是还是有所顾虑的，一个是还有另一个人，虽然没起色但是就这样平淡的有联系，虽然后续发展的可能不大，但总这样吊着也不是事儿。再说，这两个人聊天都喜欢静默个把小时才回复，真的很难受。</p><p>这个骑驴看唱本，走着瞧吧</p>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-9-1_日记</title>
      <link href="/post/55085.html"/>
      <url>/post/55085.html</url>
      
        <content type="html"><![CDATA[<h1 id="开学日记"><a href="#开学日记" class="headerlink" title="开学日记"></a>开学日记</h1><p>昨天正式开学了，过程还算顺利，就是有点儿累，不对是巨累，现在浑身酸疼。</p><p>北航开学送了点儿小礼品，还算不错，发了一件T恤，好感度方面拉满了。而且相比哈工程，北航这边的建筑明显要宏伟高大许多，道路方面也比哈工程宽敞，而且有小黄车，能骑小黄车在学校里乱跑真的不错。</p><p>宿舍里暂时只有两个人，另一个应该是大佬，拿满了奖学金本科升学来的。宿舍条件方面已经比哈工程好太多了，唯一的缺点是热水器坏了，回头去报修一下。</p><p>去了趟实验室，占了一个2080的机器，师兄师姐还是很不错的，至少氛围很轻松。听聊天内容，应该也是比较懒散的组。挺好的，当时找一个佛系导师算是找对了。</p><p>不过北京的物价确实是，啧啧，难受的很，昨天一天买点儿必需品小一百块没了，难受。(╥╯^╰╥)，食堂也比较一般。</p><p>未来可期，未来可期了属于是。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2022-8-27日记</title>
      <link href="/post/49621.html"/>
      <url>/post/49621.html</url>
      
        <content type="html"><![CDATA[<h1 id="这是一篇日常日记"><a href="#这是一篇日常日记" class="headerlink" title="这是一篇日常日记"></a>这是一篇日常日记</h1><h2 id="小小的记录一个题外话"><a href="#小小的记录一个题外话" class="headerlink" title="小小的记录一个题外话"></a>小小的记录一个题外话</h2><p>这几天有意义的工作就是帮两个师兄写了论文，其实也不算写，只能说是对原文的翻译、校对和纠错，挂了一个作者名，这算是意外之喜，但是并不能确定是不是能收录，就当是一次历练。</p><h2 id="经历"><a href="#经历" class="headerlink" title="经历"></a>经历</h2><p>这几天跟一个一起去同一个地方上学的异性初中同学有了联系，恰好开学是一个时间，于是本来打算一起买票去的，但是中途因为不可抗力的原因没有达成，这是故事的大背景。</p><p>聊天过程中，我居然有意无意地希望发展恋爱关系，这种想法很危险。不仅影响到了我的心态，也会进一步影响我的社交圈子和个人声誉。</p><p>当然在聊天过程中，我也清晰地认识到这个人跟我并不合适，甚至可以说完全不同的两个人，至少在聊天方面是大相径庭的。</p><p>我发现了自己的几个问题：</p><ol><li>长时间没有，不对，应该说，根本就没有正式进行过恋爱，导致我似乎对有可能的异性过于在意，这个同学明明可能性无限接近于0的，但是偏偏非得产生些非分之想，乃至幻想有以后种种，这简直是白日做梦的典型例子。</li><li>个人性格原因导致的聊天过程中的心理问题愈发突出。过分在意别人是不是及时回了消息，回复消息是否能达到积极性对等，甚至导致了心神不宁，只能靠游戏压制情绪的地步，这是绝对绝对的对自己心理的折磨。当然这也不是最近才出现的问题。</li></ol><p>针对这两个问题，我个人以为，要么以后杜绝网络聊天这种行为，只进行面对面交流，要么找一个跟自己性格相近的人，她应该是这样的人：</p><ol><li>在聊天时，如果遇到某些事情，会提前说明要去忙某事，过会儿回复；</li><li>如果事发突然，来不及回复就得去忙，等有时间之后会立马认真解释事情发展，诚恳说明情况；</li><li>聊天要做到有始有终，尤其是结束的时候要做到互道再见；</li></ol><p>我自认这三点按理说我自己能做得到，但是偏偏日常生活中能做到的人少之又少，而天地渺渺，又何处去找这样一个人？</p><p><img src="https://s2.loli.net/2022/08/27/UzXO6ak2NJWtFLM.png" alt="小丑聊天记录1"></p><p><img src="https://s2.loli.net/2022/08/27/MUbdCYWJkKSt9so.png" alt="小丑聊天记录2"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2022-8-13离谱bug报告</title>
      <link href="/post/21061.html"/>
      <url>/post/21061.html</url>
      
        <content type="html"><![CDATA[<h1 id="离谱BUG分析报告"><a href="#离谱BUG分析报告" class="headerlink" title="离谱BUG分析报告"></a>离谱BUG分析报告</h1><p>今天遇到了一个究极无敌离谱的GitHub的bug特此报告</p><table><thead><tr><th align="center">BUG时间</th><th align="center">2022-8-13下午</th></tr></thead><tbody><tr><td align="center">BUG内容</td><td align="center">GitHub仓库突然丢失，点击后出现page not found字样而且多次刷新后没有任何好转，在中文互联网上没有找到任何相关内容</td></tr><tr><td align="center">BUG修复</td><td align="center">重启电脑（(╥╯^╰╥)重启真的解决百分之九十九的问题）</td></tr><tr><td align="center">BUG离谱程度</td><td align="center">按理说，GitHub的仓库存储在GitHub服务器上，如果not found应该不是本地问题，但居然重启后就好了</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
            <tag> 离谱bug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习笔记</title>
      <link href="/post/6532.html"/>
      <url>/post/6532.html</url>
      
        <content type="html"><![CDATA[<h1 id="零-前言"><a href="#零-前言" class="headerlink" title="零 前言"></a>零 前言</h1><p>这是对李宏毅机器学习2021的课程笔记，一共40节课，15个大主题。</p><h1 id="一-机器学习基本概念介绍"><a href="#一-机器学习基本概念介绍" class="headerlink" title="一 机器学习基本概念介绍"></a>一 机器学习基本概念介绍</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/01_Regression_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/01_Regression_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/02_Regression_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/02_Regression_P2.pdf</a></p><h1 id="二-机器学习任务攻略"><a href="#二-机器学习任务攻略" class="headerlink" title="二 机器学习任务攻略"></a>二 机器学习任务攻略</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/03_General%20Guidance.pdf">https://github.com/goghfan/gaofansblog/blob/main/03_General%20Guidance.pdf</a></p><h1 id="三-类神经网络无法成功训练"><a href="#三-类神经网络无法成功训练" class="headerlink" title="三 类神经网络无法成功训练"></a>三 类神经网络无法成功训练</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/04_Local%20Minimum%20And%20Saddle%20Point.pdf">https://github.com/goghfan/gaofansblog/blob/main/04_Local%20Minimum%20And%20Saddle%20Point.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/05_Batch%20and%20Momentum.pdf">https://github.com/goghfan/gaofansblog/blob/main/05_Batch%20and%20Momentum.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/06_Adaptive%20Learning%20Rate.pdf">https://github.com/goghfan/gaofansblog/blob/main/06_Adaptive%20Learning%20Rate.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/07_Batch%20Normalization.pdf">https://github.com/goghfan/gaofansblog/blob/main/07_Batch%20Normalization.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/08_Classification.pdf">https://github.com/goghfan/gaofansblog/blob/main/08_Classification.pdf</a></p><h1 id="四-卷积神经网络（Convolution-Neural-Network-CNN）"><a href="#四-卷积神经网络（Convolution-Neural-Network-CNN）" class="headerlink" title="四 卷积神经网络（Convolution Neural Network CNN）"></a>四 卷积神经网络（Convolution Neural Network CNN）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/09_CNN.pdf">https://github.com/goghfan/gaofansblog/blob/main/09_CNN.pdf</a></p><h1 id="五-自注意力机制（Self-Attention）"><a href="#五-自注意力机制（Self-Attention）" class="headerlink" title="五 自注意力机制（Self-Attention）"></a>五 自注意力机制（Self-Attention）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/10_Self-attention_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/10_Self-attention_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/11_Self-attention_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/11_Self-attention_P2.pdf</a></p><h1 id="六-Transformer"><a href="#六-Transformer" class="headerlink" title="六 Transformer"></a>六 Transformer</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/12_Transformer_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/12_Transformer_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/13_Transformer_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/13_Transformer_P2.pdf</a></p><h1 id="七-生成式对抗网络GAN"><a href="#七-生成式对抗网络GAN" class="headerlink" title="七 生成式对抗网络GAN"></a>七 生成式对抗网络GAN</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/14_GAN_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/14_GAN_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/15_GAN_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/15_GAN_P2.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/16_GAN_P3.pdf">https://github.com/goghfan/gaofansblog/blob/main/16_GAN_P3.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/17_GAN_P4.pdf">https://github.com/goghfan/gaofansblog/blob/main/17_GAN_P4.pdf</a></p><h1 id="八-自监督学习（Self-Supervised-Learning与BERT）"><a href="#八-自监督学习（Self-Supervised-Learning与BERT）" class="headerlink" title="八 自监督学习（Self-Supervised Learning与BERT）"></a>八 自监督学习（Self-Supervised Learning与BERT）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/18_BERT_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/18_BERT_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/19_BERT_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/19_BERT_P2.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/20_BERT_P3.pdf">https://github.com/goghfan/gaofansblog/blob/main/20_BERT_P3.pdf</a></p><h1 id="九-自编码器（Auto-Encoder）"><a href="#九-自编码器（Auto-Encoder）" class="headerlink" title="九 自编码器（Auto-Encoder）"></a>九 自编码器（Auto-Encoder）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/21_Auto-encoder_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/21_Auto-encoder_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/22_Auto-encoder_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/22_Auto-encoder_P2.pdf</a></p><h1 id="十-恶意攻击（Adversarial-Attack）"><a href="#十-恶意攻击（Adversarial-Attack）" class="headerlink" title="十 恶意攻击（Adversarial Attack）"></a>十 恶意攻击（Adversarial Attack）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/23_Adversarial%20Attack_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/23_Adversarial%20Attack_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/24_Adversarial%20Attack_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/24_Adversarial%20Attack_P2.pdf</a></p><h1 id="十一-机器学习的可解释性（Explainable-ML）"><a href="#十一-机器学习的可解释性（Explainable-ML）" class="headerlink" title="十一 机器学习的可解释性（Explainable ML）"></a>十一 机器学习的可解释性（Explainable ML）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/25_Explainable%20AI_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/25_Explainable%20AI_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/26_Explainable%20AI_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/26_Explainable%20AI_P2.pdf</a></p><h1 id="十二-领域自适应（Domain-Adaptation）"><a href="#十二-领域自适应（Domain-Adaptation）" class="headerlink" title="十二 领域自适应（Domain Adaptation）"></a>十二 领域自适应（Domain Adaptation）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/27_Domain%20Adaptation.pdf">https://github.com/goghfan/gaofansblog/blob/main/27_Domain%20Adaptation.pdf</a></p><h1 id="十三-强化学习（Reinforcement-Learning-RL）"><a href="#十三-强化学习（Reinforcement-Learning-RL）" class="headerlink" title="十三 强化学习（Reinforcement Learning RL）"></a>十三 强化学习（Reinforcement Learning RL）</h1><p><a href="https://github.com/goghfan/gaofansblog/blob/main/28_Reinforcement%20Learning_P1.pdf">https://github.com/goghfan/gaofansblog/blob/main/28_Reinforcement%20Learning_P1.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/29_Reinforcement%20Learning_P2.pdf">https://github.com/goghfan/gaofansblog/blob/main/29_Reinforcement%20Learning_P2.pdf</a></p><p><a href="https://github.com/goghfan/gaofansblog/blob/main/30_Reinforcement%20Learning_P3.pdf">https://github.com/goghfan/gaofansblog/blob/main/30_Reinforcement%20Learning_P3.pdf</a></p><h1 id="十四-终身学习（Life-Long-Learning-LL）"><a href="#十四-终身学习（Life-Long-Learning-LL）" class="headerlink" title="十四 终身学习（Life Long Learning LL）"></a>十四 终身学习（Life Long Learning LL）</h1><h1 id="十五-元学习（Meta-Learning）"><a href="#十五-元学习（Meta-Learning）" class="headerlink" title="十五 元学习（Meta Learning）"></a>十五 元学习（Meta Learning）</h1>]]></content>
      
      
      <categories>
          
          <category> 李宏毅机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 李宏毅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交叉熵与机器学习</title>
      <link href="/post/4698.html"/>
      <url>/post/4698.html</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>交叉熵（Cross Entropy）是深度学习中常用的一个概念，一般用来求目标和预测值之间的差距。大家常做的是直接调用现有的库，用起来很方便，但是对于为什么要用这个，这个的原理是什么其实并不清楚，这里梳理一下相关知识。</p><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p>交叉熵其实是信息论中的一个概念，想要了解交叉熵的本质，需要从基本的概念讲起。</p><h2 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h2><p>假设我们现在有两件事儿，分别如下：</p><ul><li>事件A：巴西队进入世界杯决赛</li><li>事件B：中国队进入世界杯决赛</li></ul><p>显然，当上面两件事情发生的时候，我们会下意识的觉得，巴西队进世界赛理所当然，所以应该是一路平推，连战连胜进入决赛了，中国队能进世界赛？这其中发生了什么惊天逆转？是不是有球员究极爆发？是不是对方收钱了等等等的一些列疑问。</p><p>究其原因，是因为A事件发生的概率大，而B事件发生的概率小，当不太可能的事情发生的时候，我们就能获得更多的信息。越是可能的事情发生了，我们对这件事能获得的信息量就越小。因此，信息量应该跟发生的概率相关。</p><p>假设X是一个离散型随机变量，其取值集合为$a+b$, 概率分布函数$p(x)=Pr(X=x),x\in\chi,x∈χ$,则定义事件X=x0的信息量为：<br>$$<br>I(x_0)=-log(p(x_0))<br>$$<br>由于是概率，因此$p(x_0)$的取值范围应该在[0,1]，那么其函数应该为：</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://s1.ax1x.com/2022/07/30/viZQFx.png"></div></div><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>考虑一个问题，对于某个事件，有n种可能，每种可能性都有一个概率 P(x)</p><p>这样就可以计算出某一种可能性的信息量，举例说，假设拿出电脑，摁下开机，会有如下三种事件和概率：</p><table><thead><tr><th align="center">序号</th><th align="center">事件</th><th align="center">概率</th><th align="center">信息量</th></tr></thead><tbody><tr><td align="center">A</td><td align="center">正常开机</td><td align="center">0.7</td><td align="center">-log（P(A)）=0.36</td></tr><tr><td align="center">B</td><td align="center">无法开机</td><td align="center">0.2</td><td align="center">-log（P(B)）=1.61</td></tr><tr><td align="center">C</td><td align="center">电脑爆炸</td><td align="center">0.1</td><td align="center">-log（P(C)）=2.30</td></tr></tbody></table><p>用熵来<strong>表示所有信息量的期望</strong>，公式如下：<br>$$<br>H(X)=-\sum_{i=1}^n p(x_i)log(p(x_i))<br>$$<br>其中n表示所有的可能事件，所以上面问题的结果就是：<br>$$<br>\begin{eqnarray}<br>H(X)&amp;=&amp;-[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))]\<br>&amp;=&amp;0.7\times 0.36+0.2\times 1.61+0.1\times 2.30\<br>&amp;=&amp;0.804<br>\end{eqnarray}<br>$$<br>当然，如果事件更少，比如扔硬币，只有正面和反面两种情况（我们暂时不考虑正好站起来的情况），那么这样的公式将更加简单。</p><h3 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h3><p>相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异</p><p>在wiki百科里面对相对熵的定义是：</p><pre class="line-numbers language-none"><code class="language-none">In the context of machine learning，DKL is often called the information gain achieved if Pis used instead of Q<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>也就是</p><pre class="line-numbers language-none"><code class="language-none">如果用P来描述目标问题，而不是Q来描述目标问题，得到的信息增量<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在ML中，P往往用样本的真实分布，比如[1,0,0]表示当前的样本属于第一类。Q表示模型所预测的分布，比如[0.7,0.2,0.1]。直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是没有那么真实，或者说没有那么完美，信息量不足，需要额外的一些<strong>“信息增量”</strong>才能达到P一样的完美描述的效果。如果我们用Q通过反复训练，也能完美的描述样本，那么就不需要额外的增量，Q就等价于P。</p><p>KL散度计算的公式为：<br>$$<br>D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{3.1}<br>$$<br>n为所有事件的可能性</p><p>DKL的值越小，Q和P分布就越接近，训练得到的Q的效果就越好</p><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>对3.1公式变形，可以推理得到下面的公式：<br>$$<br>\begin{eqnarray}<br>D_{KL}(p||q) &amp;=&amp; \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\<br>&amp;=&amp; -H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]<br>\end{eqnarray}<br>$$<br>等式的前一部分就是P的熵，等式后一部分就是交叉熵：<br>$$<br>H(p,q)=-\sum_{i=1}^np(x_i)log(q(x_i))<br>$$<br>在机器学习中，我们需要评估lable和predicts之间的差距，使用KL散度就正好满足要求，由于KL散度中的前一部分-H(p(x))不变，因此在优化过程中，只需要关注交叉熵就可以了，所以交叉熵也常用于机器学习的Loss函数。</p><h1 id="机器学习中交叉熵的应用"><a href="#机器学习中交叉熵的应用" class="headerlink" title="机器学习中交叉熵的应用"></a>机器学习中交叉熵的应用</h1><h2 id="为什么使用交叉熵"><a href="#为什么使用交叉熵" class="headerlink" title="为什么使用交叉熵"></a>为什么使用交叉熵</h2><p>在线性回归的问题中，常常使用MSE作为损失函数，如：<br>$$<br>loss = \frac{1}{2m}\sum_{i=1}^m(y_i-\hat{y_i})^2<br>$$<br>这里的m表示m个样本</p><p>MSE常用在线性问题中，但是在逻辑分类问题中还好用么</p><p>交叉熵在单类别问题中的应用</p><p>这里单类别指的是，每张图片样张只有一个类别，比如只能是狗，或者只能是猫</p><p>交叉熵在单分类问题上基本是标配的方法：<br>$$<br>loss=-\sum_{i=1}^{n}y_ilog(\hat{y_i}) \tag{2.1}<br>$$<br>上式为一张样本的loss计算方法。式2.1中n代表着n种类别。<br>举例说明,比如有如下样本</p><p><img src="https://img-blog.csdn.net/20180125164444783?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHN5Y2NuaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>对应的标签和预测值</p><table><thead><tr><th align="center">*</th><th align="center">猫</th><th align="center">青蛙</th><th align="center">老鼠</th></tr></thead><tbody><tr><td align="center">Label</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="center">Pred</td><td align="center">0.3</td><td align="center">0.6</td><td align="center">0.1</td></tr></tbody></table><p>那么对应的损失函数就是：<br>$$<br>\begin{eqnarray}<br>loss&amp;=&amp;-(0\times log(0.3)+1\times log(0.6)+0\times log(0.1)\<br>&amp;=&amp;-log(0.6)<br>\end{eqnarray}<br>$$<br>对应一个batch 的loss就是<br>$$<br>loss=-\frac{1}{m}\sum_{j=1}^m\sum_{i=1}^{n}y_{ji}log(\hat{y_{ji}})<br>$$</p><h2 id="交叉熵在多分类问题中的应用"><a href="#交叉熵在多分类问题中的应用" class="headerlink" title="交叉熵在多分类问题中的应用"></a>交叉熵在多分类问题中的应用</h2><p>这里多分类是指，一个图像样本会含有多个类别，例如同时含有一只猫和一只狗</p><p><img src="https://img-blog.csdn.net/20180125164456925?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHN5Y2NuaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>对应的标签和预测就是：</p><table><thead><tr><th align="center">*</th><th align="center">猫</th><th align="center">青蛙</th><th align="center">老鼠</th></tr></thead><tbody><tr><td align="center">label</td><td align="center">0</td><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">pred</td><td align="center">0.1</td><td align="center">0.7</td><td align="center">0.8</td></tr></tbody></table><p>为什么这里的预测概率不再和为1了呢？因为这里的Pred不再是由softmax计算的，而是使用sigmod，将每个节点的输出归一化到[0,1]之间，和也不再为1.也就是说每个Label都是独立分布的，互相之间没有任何关系，所以交叉熵在这里对每一个节点进行计算，每个节点的值是二项分布。</p><p>那么简化一下二项分布的loss为：<br>$$<br>loss =-ylog(\hat{y})-(1-y)log(1-\hat{y})<br>$$<br><strong>但是一定注意这里的loss只是一个节点的loss，而非全局的loss</strong></p><p>正式的写法应该是：<br>$$<br>\begin{eqnarray}<br>loss_猫 &amp;=&amp;-0\times log(0.1)-(1-0)log(1-0.1)=-log(0.9)\<br>loss_蛙 &amp;=&amp;-1\times log(0.7)-(1-1)log(1-0.7)=-log(0.7)\<br>loss_鼠 &amp;=&amp;-1\times log(0.8)-(1-1)log(1-0.8)=-log(0.8)<br>\end{eqnarray}<br>$$<br>单张图片的loss就是：$ loss= loss_猫+loss_狗+loss_蛙 $</p><p>每个batch的loss就是<br>$$<br>loss =\sum_{j=1}^{m}\sum_{i=1}^{n}-y_{ji}log(\hat{y_{ji}})-(1-y_{ji})log(1-\hat{y_{ji}})<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>极大似然估计和贝叶斯估计</title>
      <link href="/post/58164.html"/>
      <url>/post/58164.html</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="极大似然估计与贝叶斯估计"><a href="#极大似然估计与贝叶斯估计" class="headerlink" title="极大似然估计与贝叶斯估计"></a>极大似然估计与贝叶斯估计</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>极大似然估计和贝叶斯估计都是统计模型中两种对模型参数的确定方法。前者属于概率派，认为参数是固定的，我们要做的事情就是根据已经掌握的数据来估计这个参数；后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的。<strong>因此，简单说，极大似然估计就是假设一个参数θ，然后根据数据来求出这个θ. 而贝叶斯估计的难点在于p(θ) 需要人为设定，之后再考虑结合MAP （maximum a posterior）方法来求一个具体的θ.</strong></p><p>因此，两者最大的不同在于<strong>是否考虑了先验</strong>，两者的使用范围也就成为了：<strong>前者适用于数据大量，估计的参数能够较好地反映实际情况，而贝叶斯估计则是在数据量较少的情况下或者比较稀疏的情况下，考虑先验来提升准确率。</strong></p><h2 id="预知识"><a href="#预知识" class="headerlink" title="预知识"></a>预知识</h2><p>为了更好的讨论，本节会先给出我们要解决的问题，然后给出一个实际的案例。这节不会具体涉及到极大似然估计和贝叶斯估计的细节，但是会提出问题和实例，便于后续方法理解。</p><h3 id="问题前提"><a href="#问题前提" class="headerlink" title="问题前提"></a>问题前提</h3><p>首先，我们有一堆数据<br>$$<br>D={x_1,x_2,…,x_n}<br>$$<br>当然这些数据不是随便产生的，我们假设这些数据是以含有未知参数θ某种概率形式分布的。我们的任务就是通过已有的数据来估计这个未知参数θ，估计这个参数的好处就在于，我们可以对外来的数据进行预测。</p><h3 id="问题实例"><a href="#问题实例" class="headerlink" title="问题实例"></a>问题实例</h3><p>假设我们抛硬币，抛硬币之前我们不知道这些硬币是不是均匀分布的正反面，也许正反面不等，我们假设正面向上设为1的概率为ρ，反面向上设为0概率为（1-ρ），我们进行了三次实验，得到两次正面，一次反面，也就是得到的次序为1 1 0，那么这里的D就是{1,1,0}，θ=ρ。</p><h3 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h3><p>这里有些符号方便我们下面对方法进行解释。</p><table><thead><tr><th align="center">符号</th><th align="center">解释</th></tr></thead><tbody><tr><td align="center">D</td><td align="center">已有的数据</td></tr><tr><td align="center">θ</td><td align="center">要估计的参数</td></tr><tr><td align="center">p(θ)</td><td align="center">先验概率</td></tr><tr><td align="center">p(θ|D)</td><td align="center">后验概率</td></tr><tr><td align="center">p(D)</td><td align="center">数据分布</td></tr><tr><td align="center">p(D|θ)</td><td align="center">似然函数</td></tr><tr><td align="center">p(x,θ|D)</td><td align="center">已知条件下x,θ的概率</td></tr></tbody></table><h2 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h2><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>极大似然估计方法认为参数是固有的，但是可能由于一些外界的干扰，让数据看起来不完全是由参数决定的。但是科学家认为，虽然有误差，但是只要让这个数据在给定情况下，找到一个概率最大的参数就可以了。那么问题就可以转变成一个条件概率求最大解的情况，即求p(θ|D)最大的参数θ，形式化表达式为：<br>$$<br>argmax_θp(θ|D)<br>$$<br>而根据条件概率公式：<br>$$<br>p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}<br>$$<br>因为我们在极大似然估计中假设θ是确定的，所以p(θ)是一个常数，p（D）同样是根据已有的数据得到的，也是确定的，或者我们可以把其看做是对整个概率的一个归一化因子，这时候求解上述公式就变成了<br>$$<br>\arg\max_{\theta}p(D|\theta)<br>$$<br>其中的P（D|θ）就是似然函数，我们要做的就是求一个似然最大的参数，所以成为极大似然估计。</p><p>想求解这个问题，需要假设我们的数据是相互独立的，<br>$$<br>D={x_1,x_2,…,x_n}<br>$$<br>这是有：<br>$$<br>p(D|\theta)=\prod_{i=1}^{n}p(x_i|\theta)<br>$$<br><strong><u>一般来说，要对上式取对数求解对数极大似然，就可以把连乘变成连加，然后求导取极值点就是要求的参数值了。</u></strong></p><h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>考虑到这一节中会多次提到先验概率这个概念，我们首先介绍先验和后验的区别，怎么得到的，其次介绍贝叶斯估计模型的推导过程，最后举例说明。</p><h4 id="先验概率和后验概率"><a href="#先验概率和后验概率" class="headerlink" title="先验概率和后验概率"></a>先验概率和后验概率</h4><p>我们简称上面两个概率为先验和后验，这两个概念其实来自贝叶斯定理。</p><p>先验概率和后验概率是相比较而言的，这个被用来比较先后的对象就是参数θ。后验概率是指掌握了一定量的数据后我们的参数分布是怎么样的，表示为p(θ|D)；那先验就是在没有掌握数据前，我们的参数该如何分布。</p><p>但问题来了，如果没有数据，那我们怎么推断参数如何分布？提出这个问题的，往往被认为是一个概率派学家，而不是贝叶斯派学家，贝叶斯估计最重要的就是先验的获得过程，虽然你这次的一组数据，比如扔硬币产生的序列110，<strong>但是其实我根据历史的经验来看</strong>，一枚硬币的正反很可能是均匀分布的，只不过可能因为<strong>实验次数太少</strong>所以没有得到理想数据，所以我们要利用历史经验来假设。</p><p>对！你没听错，就是猜测。先验在很多时候完全是猜测的假设，然后验证数据是否符合这个猜想，所以猜是很重要的一环。还要注意，<strong>先验是跟数据完全，完全，完完全全没有关联的</strong>，你不能有了数据再猜想，必须是没有任何数据就进行猜测一个先验概率。</p><h4 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h4><p>$$<br>p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}<br>$$</p><p>这个公式其实是很概括的模型，没有对概率参数进行定义，也没有运用到参数固定与否的思想，因此这也适用于贝叶斯模型。</p><p>此时，这里除了分母可以看做一个归一化因子之外，其余均为概率分布的函数，也就是说，不能再把p(θ)当做一个常量，这是极大似然估计的做法，这时候就要用到我们的先验概率了。我们利用全概率公式把分母展开，得到：<br>$$<br>p(D)=\int_{\theta}p(D|\theta)p(\theta)d\theta<br>$$<br>然后将下面的式子和上面的式子一起带入最开始的条件概率公式<br>$$<br>p(D|\theta)=\prod_{i=1}^{n}p(x_i|\theta)<br>$$<br>得到:<br>$$<br>p(\theta|D)=\frac{(\prod_{i=1}^{n}p(x_i|\theta))p(\theta)}{\int_{\theta}(\prod_{i=1}^{n}p(x_i|\theta))p(\theta)d\theta}<br>$$<br>那么，我们完成了贝叶斯模型的推导过程，虽然这个式子非常复杂，但是其实仔细观察，每个数值和符号都是已知的。下面我们举个例子。</p><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>我们得到的最终模型，根据之前对先验的介绍，这是在没有数据之前我们就已经知道的函数了。知道是什么意思？不妨还是在那个抛硬币试验中，我们假设这个θ(ρ)的先验概率是服从下面这个式子的概率分布的，<br>$$<br>f_\rho(\rho)=6\rho(1-\rho)<br>$$<br>如图：</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://s1.ax1x.com/2022/07/25/jxFOiR.png"></div></div><p>那么，<br>$$<br>\prod_{i=1}^{n}p(x_i|\theta)=ρ<em>ρ</em>(1-ρ)<br>$$<br>接下来，将所有内容带入到我们推导的公式中。</p><p>但是，其实做到这一步，我们会发现虽然解决了问题，但是又会带来新的问题，因为在解决这一类贝叶斯估计的问题的时候，我们让参数以某种概率密度函数分布，就会导致在计算过程中不可避免的高复杂度，<strong>人们为了计算上的方便，就提出不再是把所有的后验概率p(θ|D)都找出来，而是仍然采用类似于极大似然估计的思想，来极大后验概率(Maximum A Posterior)，得到这种简单有效的叫做MAP（前面英文的首字母）的算法。下面我们再一步步介绍一下MAP。</strong></p><h4 id="极大后验概率MAP"><a href="#极大后验概率MAP" class="headerlink" title="极大后验概率MAP"></a>极大后验概率MAP</h4><p>MAP（Maximum A Posterior）的理论依据是绝大部分情况下，参数值最有可能出现在概率最大点附近。为了说清楚MAP的来龙去脉，本节将首先介绍如何利用贝叶斯估计的参数进行预测，然后分析直接使用之前得到的后验概率有什么不好，最后介绍MAP算法做的工作。</p><h5 id="使用贝叶斯估计的参数做预测"><a href="#使用贝叶斯估计的参数做预测" class="headerlink" title="使用贝叶斯估计的参数做预测"></a>使用贝叶斯估计的参数做预测</h5><p>前一节中，我们通过贝叶斯估计得到了后验概率p(θ|D)。那么这个后验概率能用来做什么呢？当然，就比如我们一直在说的那个例子，得到了数据D=(110)，还想预测第四次得到的结果什么是什么怎么办？我们当然就需要计算p(0|D)和p(0|D)看看谁大谁小，哪个更有可能发生。这里，为了泛化，我们将问题再次形式化一下为</p><p><strong>已知数据D=(x1,x2,…,xn)，预测新的数据x的值。</strong></p><p>这个问题还有很多细节，比如先验概率，后验概率，数据分布等一些细节，因为前面已经介绍过了，这里为了突出重点，不再重复。在此需要关注的是，所谓预测新的数据的值，其实就是能够在已知数据D的情况下，找到数据的数学期望。即求<br>$$<br>E(x|D)=\int_xxp(x|D)dx.<br>$$<br>也就是我们需要求 p(x|D)，这该怎么办？其实这个式子比较迷惑人的点就在于，它内藏了一个参数，也就是x的分布其实与参数是有关的，但是又参数 θ是服从某种概率分布的，要对参数所有可能的情况都考虑就得到了<br>$$<br>p(x|D)=\int_{\theta}p(x,\theta|D)d\theta<br>$$<br>这一式子。<br>接下来还是运用基本的条件概率公式<br>$$<br>p(x,\theta|D)=p(x|\theta,D)p(\theta|D).<br>$$<br>对这一句公式的解释就是， x和θ在已知数据 D的条件下的概率，等于x在已知 θ和数据 D的条件下的概率乘θ在已知数据 D的条件下的概率。为什么我要费这个心来说这个，一方面是我为了方便大家理解这个多维条件概率符号的含义，另一方面更重要的是右边式子的第一项p(x|θ,D)可这样<br>$$<br>p(x|\theta,D)=p(x|\theta)<br>$$<br>化简。为什么？因为我们从数据里面得到的东西对一个新的数据来说，其实只是那些参数，所以对 x而言，θ就是 D，两者是同一条件。<br>那么(10)式就变成了<br>$$<br>p(x|D)=\int_{\theta}p(x,\theta|D)d\theta=\int_{\theta}p(x|\theta)p(\theta|D)d\theta<br>$$<br>p(x|θ)是已知的( 例如在我们的问题里面可以是p(1|ρ)或者p(0|ρ))； p(θ|D)也是已知的，我们在贝叶斯估计中已经求出来了。所以这个式子完全就是一个只含有x的函数，完全可以计算出来数学期望。但是！这里面我忽略了一个事实，这里面存在什么困难呢？下面会帮助大家分析。</p><h5 id="贝叶斯估计的最后一个困难"><a href="#贝叶斯估计的最后一个困难" class="headerlink" title="贝叶斯估计的最后一个困难"></a>贝叶斯估计的最后一个困难</h5><p>这里面的困难是参数是随机分布的，我们需要考虑到每一个可能的参数情况然后积分，这种数学上的简单形式，其实想要计算出来需要大量的运算。那我们不妨退而求其次，我找一个跟你差不多效果的后验概率，然后就只计算这个后验带入计算。那么什么样的后验概率和对所有可能的θ积分情况差不多呢？想法就是，找一个θ能够最大化后验概率，怎么才能最大化后验概率呢？</p><h5 id="MAP算法"><a href="#MAP算法" class="headerlink" title="MAP算法"></a>MAP算法</h5><p>使用<br>$$<br>\theta_{MAP}=\arg\max_{\theta}\prod_{i=1}^{n}p(x_i|\theta)p(\theta)<br>$$<br>这其实与极大似然估计形式上很相似，但是主要区别在于运用了一个先验概率在这个极大化里面。参数都已经计算出来了，其他过程，其实还是按照极大似然来做就行了，不用再按照贝叶斯一样对所有可能的参数情况都考虑在求积分了。</p>]]></content>
      
      
      <categories>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-7-20 日记</title>
      <link href="/post/59674.html"/>
      <url>/post/59674.html</url>
      
        <content type="html"><![CDATA[<h1 id="这是一篇日记"><a href="#这是一篇日记" class="headerlink" title="这是一篇日记"></a>这是一篇日记</h1><h2 id="心情"><a href="#心情" class="headerlink" title="心情"></a>心情</h2><p>这几天经历了一点点事情，让我对自己的聊天观感产生了深深的怀疑。其实也不是这几天才有，以前一直对自己聊天，尤其是网上聊天的心态有诸多不理解之处。</p><p>就我个人来说，无论线上线下聊天都想的是有始有终，最后都会说完再见才算整个聊天完整的结束了。</p><p>在生活中聊天这种问题是自然而然的，离开之后，也就算是互相地告别了，也就标志着聊天结束了。但是线上因为不知道对方在干什么，从而就无法判断是不是该继续聊下去。当我这里说完了，我希望的是，哪怕没有后续了，对方也会回复好的，收到，或者嗯嗯，这类表示已经结束的对话。但是似乎网络上大部分人对结束对话的定义与我大相径庭，他们似乎认为：奥，你说完了，那结束了啊，还回复什么呢，麻烦。</p><p>当然大部分人的这种态度对我来说是无所谓的，但是一旦觉得对方可能是自己潜在的另一半的时候，或者说是自己很在意的异性的时候，就很难以接受这种态度，觉得太过敷衍，不，对方甚至是懒得敷衍。或者再扩大一些，不仅仅是最后对话结束的时候没有回应让我很难受，而是在聊天中莫名其妙地突然消失一段时间，而且没有任何理由的时候，我都会为这种态度困扰。这就像是编程的时候，我们一起打开了一个文件，但是必须得对方关闭文件才能消除这个文件的进程。于是，一旦对方没有征兆的失联，没有道别后结束聊天，那么这个进程就会一直困扰我，提醒我：这件事情没结束！没结束！可能他马上就回来了！你得时刻保持警惕！也就导致我被这种烦不胜烦的消息一直牵动着心绪，直到自己疲惫不堪，心情不堪重负，随着时间的推移忘记这件事才行。而这段时间又偏偏特别长。这就导致我的心情一直被这个进程绑架，无所遁形。</p><p>也许这种心态是错误的，但是我无可避免。似乎唯一的方法就是不去聊天，不打开这个进程。</p>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记1</title>
      <link href="/post/42504.html"/>
      <url>/post/42504.html</url>
      
        <content type="html"><![CDATA[<h1 id="On-the-Principles-of-Parsimony-and-Self-Consistency-for-the-Emergence-of-Intelligence（论智能产生的简约和自洽原则）"><a href="#On-the-Principles-of-Parsimony-and-Self-Consistency-for-the-Emergence-of-Intelligence（论智能产生的简约和自洽原则）" class="headerlink" title="On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence（论智能产生的简约和自洽原则）"></a>On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence（论智能产生的简约和自洽原则）</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>作者提出了两个原则简约和自洽作为人工智能或者自然智能的产生基础，并认为这两个原则产生了一个有效、高效的计算框架—压缩比环转录。</p><p>第 1 节主要介绍研究背景和动机；</p><p>第 2 节使用可视化数据建模作为具体示例来提出两个原则——简约和自洽，并说明如何将它们实例化为可计算的目标、架构和系统；</p><p>第 3 节研究者推测这两个原则会使得通用学习引擎用于更广泛的感知和决策任务；</p><p>第 4 节研究者讨论了所提出原则的多层含义及其与神经科学、数学和高级智能的联系。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>再过去的十年里，人工智能的进展很大程度上依赖蛮力方法训练同质黑河模型，如深度神经网络。虽然功能模块化可能出现在训练中，但学习到的特征表示在很大程度上仍然是隐藏的、潜在的、并且是难以解释的。这种端到端的黑盒模型不仅导致了不断增长的模型大小和数据计算成本，而且有很多问题不可解释，缺乏丰富性、缺乏稳定性、缺乏适应性和对灾难性遗忘的易感性、缺乏对抗变形的鲁棒性。</p><p>我们假设<em><strong>当前深度网络和人工智能实践中出现这些问题的根本原因之一是对智能系统的功能和组织原则缺乏系统和综合的理解。</strong></em></p><p>例如，在实践中，训练用于分类的判别模型和用于采样的生成模型很大程度上是分开的，这类系统通常是开环系统，需要通过监督或者自监督进行段导弹的训练。在控制论中，研究者长期坚持的一个原则是，<strong>这种开环系统不能自动纠正预测中的错误，并且不能适应环境的变化。</strong>研究者将闭环反馈引入受控系统，以便系统能够学会纠正其错误。正如本文所讨论的，一旦将判别模型和生成模型组合在一起形成一个完整的闭环系统，学习就变得自主（无需外部监督），并且更高效、稳定且适应性强。</p><p>要理解智能系统中可能需要的功能部件，例如判别式或者生成式，研究者需要从原则性且统一的角度来理解智能。为此，这篇文章我们提出了两个基本原则：<em><strong>简约和自洽，</strong></em>我们相信它支配了所有智能系统的功能和设计，无论人工或者自然的。</p><p>这两个原则也用于回答两个基本的学习问题：</p><ol><li>学习什么：从数据中学习的目标是什么，如何衡量？</li><li>如何学习：我们如何通过高效和有效的计算来实现这样的目标？</li></ol><p>第一个问题的答案属于<strong>信息/编码理论领域</strong>，该理论研究如何准确量化和测量数据的信息，然后寻求信息的最紧凑表示。一旦学习的目标明确并确定，第二个问题的答案自然会落到<strong>控制/博弈领域，</strong>该领域提供了一个有效普遍的计算框架，也就是<strong>闭环反馈系统</strong>，用于一致地实现可测量目标。如图所示。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled.png)</p><p>(左侧为具有低纬度结构的外部多模态高纬度感官数据)（右侧为具有紧凑和结构化表示的内部表示）</p><h1 id="智能的两个原则"><a href="#智能的两个原则" class="headerlink" title="智能的两个原则"></a>智能的两个原则</h1><p>本节中将详细解释回答两个问题，what to learn 和 how to learn</p><h2 id="学习什么：简化原则"><a href="#学习什么：简化原则" class="headerlink" title="学习什么：简化原则"></a>学习什么：简化原则</h2><p>简约原则：对于一个智能体系来说学习的目的是<strong>在对外界的观察中确定低维结构，并以最紧凑和结构化的方式重新组织它们。</strong></p><p>为什么智能系统需要体现这个原则的根本原因是：没有它，智能就不可能存在，如果对于外部世界的观察没有低纬结构，那么久没有任何值得学习或者记忆的东西，没有任何东西可以依赖于良好的推广或者预测，这两者都依赖于与同样的低纬结构下进行的新观测，因此，这个原则不仅仅是一种便利，因为智能系统都需要节约能源、空间、时间登资源</p><p>上述原则也可以称为<strong>压缩原则</strong>。但是智能中的简约并不是实现可能的最好的压缩，而是获取<em><strong>通过有效的计算手段获得紧凑的结构化表示。智能系统试图将数据压缩到柯式复杂度（Kolmogorov complexity）或香农信息（Shannon information）的最高水平是无意义的。</strong></em></p><p>现在我们面临一个问题：<em><strong>一个智能系统如何体现简约原则，以一种计算可处理甚至有效的方式来识别和表示观测中的结构?</strong></em></p><p>从理论上讲，智能系统可以使用世界上任何理想的结构化模型系列，只要它们简单且足够表达以模拟现实世界感官数据中的有用结构。系统应该能够准确有效地评估学习模型的好坏，并且使用的度量应该是基本的、通用的、易于计算和优化的。该研究使用了可视化数据建模的激励性和直观示例。</p><p>正如我们将在下一节看到的，以节俭作为唯一的“自我监督”，加上自我一致性的第二个原则，一个学习系统可以成为完全自主和功能，而不需要任何外部监督。</p><p><strong>建模和计算简约</strong></p><p>如下图所示，x 表示输入的传感数据，比如一个图像，用 z 来表示它的内部表示。传感数据样本 x ∈ R^D 通常是相当高维的（数百万像素），但具有极低维的内在结构。在不损失一般性的情况下，我们可以假设它分布在一些低维子流形上，如图 2 所示。其学习的目的是建立一个（通常是非线性的）映射f，比如在一个参数族Θ，从x到一个更低维度的表示z∈Rd （x ∈RD - - -f(x,θ)−−→z ∈Rd）使得特征 z 的分布更加紧凑和结构化。<em><strong>紧凑意味着储存的经济性。结构化意味着访问和使用的效率:</strong></em> 特别是，线性结构是插值或外推的理想选择。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 1.png)</p><p>更具体、更精确地说，我们可以将用于可视化数据建模的Parsimony原则正式实例化为试图找到一个(非线性)变换f来实现以下目标:</p><ol><li>压缩：将高维感官数据 x 映射到低维表示 z；</li><li>线性化：将分布在非线性子流形上的每一类对象映射到线性子空间；</li><li>划痕（scarification）：将不同的类映射到具有独立或最大不连贯基础的子空间。</li></ol><p>也就是将可能位于高维空间中的一系列低维子流形上的真实世界数据分别转换为独立的低维线性子空间系列。这种模型称为<strong>“线性判别表示”（linear discriminative representation，LDR）</strong>，压缩过程如图上图所示。</p><p><strong>最大限度地降低利用率</strong></p><p>对于 LDR 模型系列，有一种<strong>自然的内在简约度量</strong>。直观地说，给定一个 LDR，我们可以<strong>计算所有子空间上的所有特征所跨越的总体积以及每个类别的特征所跨越的体积之和。然后这两个体积之间的比率给出了一个自然的衡量标准，来表明 LDR 模型性能：比率越大越好。</strong>图 3 展示了一个示例，其中特征分布在两个子空间 S1 和 S2 上。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 2.png)</p><p>左侧和右侧的模型具有相同的内在复杂性。显然，左侧的配置是首选，因为不同类别的特征是独立且正交的——它们的外部表征将是最稀疏的。因此，根据这个基本的体积测量，最好的表示应该是“整体最大值大于各部分之和”</p><p>根据信息论，分布的体积可以通过其速率失真来衡量。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 3.png)</p><p>这个公式给出了一个最基本的，类似于bean计数，用于衡量Z特征的算法，如上图</p><p>尽管对于高维空间中的一般分布，与前面提到的许多其他测量一样，速率失真是难以处理的，实际上是 NP 难以计算的(MacDonald 等，2019) ，但是从子空间上支持的高斯得到的数据 Z 的速率失真具有封闭形式的公式(Ma 等，2007)</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 4.png)</p><p>因此这是可以有效计算和优化的</p><p>Chan 等人(2022)的工作表明，如果一个人使用高斯率失真函数，并选择一个通用的深层网络(比如 ResNet)来建模映射 f (x，θ) ，然后通过最大化编码速率降低，称为 MCR^2原理:</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 5.png)</p><p>可以有效地将多类可视数据集映射到多个正交子空间。注意，最大化速率降低 R 的第一项会扩展所有特性的体积。也就是说，它同时对所有特征进行“对比学习”，这可以比传统对比方法中通常进行的对比样本对有效得多(Hadsell 等，2006; Oord 等，2018)。最小化第二学期 Rc 压缩和线性化每个类中的特性。这可以解释为对每个班级进行“收缩性学习”(Rifai et al。 ，2011)。利率降低目标统一并推广了这些启发式算法。</p><p>特别是，人们可以严格地表明，<strong>通过最大限度地降低速率，不同类的特征将是独立的，每个类的特征将几乎均匀地分布在每个子空间中(Chan等人，2022)。相反，将每个类映射到一个热点标签的广泛实践的交叉熵目标将每个类的最终特征映射到一维单例</strong>(Papyan等人，2020)。</p><p><strong>对白盒深度网络展开最优化</strong></p><p>请注意，在这种情况下，深度网络的作用只是简单地为外部数据x和内部表示z之间的非线性映射f建模。一个智能系统首先应该如何知道为映射f使用什么模型族呢?有没有一种方法可以直接推导和构建这样的映射，而不是猜测和尝试不同的可能性?</p><p>回想一下，我们的目标是优化速率下降∆R(Z)作为特征集Z的函数。为此，我们可以直接从原始数据Z0=X开始，逐步优化R（Z）比如采用预测梯度上升(PGA)方案</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 6.png)</p><p>也就是说，我们可以跟随速率降低的梯度来移动这些特征，从而使速率降低增加。这种基于梯度的迭代变形过程如图4所示。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 7.png)</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 8.png)</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 9.png)</p><p>其中 El 和 Cl 是线性算子，完全由前一层 Zl的特征的协方差决定。在这里，σ 是一个软极大算子，它根据 z‘ s 到每个类的距离(由 C‘ l’度量)将 z‘赋给最接近的类。图5左侧给出了每次迭代的所有操作符的关系图。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 10.png)</p><p><strong>正向展开而不是反向传播</strong></p><p>我们在上面看到，压缩导致了一个完全建设性的方式来产生一个深层神经网络，包括其结构和参数，作为一个完全可解释的白盒子: 其层次进行迭代和增量优化的原则目标，促进简约。</p><p>因此，对于所获得的深层网络，ReduNet 从数据 X 作为输入开始，以一种完全正向展开的方式构造和初始化每一层的运算符和参数(E‘ ，C‘)。这与深度学习中流行的做法非常不同: 从一个随机构造和初始化的网络开始，然后通过反向传播进行全局调整(Rumelhart et al。 ，1986)。人们普遍认为，由于对对称突触的要求和反馈的复杂形式，大脑不太可能利用反向传播作为其学习机制。</p><p>在这里，正向展开操作类似地，展开迭代优化序列稀疏恢复导致经常性网络(Wisdom 等，2017)。时间化只依赖于可以硬连接的相邻层之间的操作; 因此，自然界更容易实现和利用时间化。此外，这种网络的参数和操作符可以通过另一种优化水平进行进一步的微调，例如，通过反向传播实现的(随机)梯度下降(Rumelhart et al 1986)。但是不应该混淆用于微调网络的(stochas-tic)梯度下降法和网络层应该实现的基于梯度的优化。</p><p><strong>平移不变性和非线性</strong></p><p>如果我们进一步希望学习编码 f 对所有的时间移位或空间移位都是不变的(或等变的) ，我们将每个样本 x (t)的所有移位版本{ x (t-τ)<strong>∀</strong> τ }看作是同一个等价类。如果我们把它们压缩和线性化到同一个子空间，那么上述梯度运算中的所有线性算子 E 或 C 自动成为多通道卷积！因此，ReduNet 自然而然地成为一个多通道卷积神经网络(CNN) ，最初提出用于平移不变识别</p><p><strong>神经网络的人工选择和进化</strong></p><p>一旦我们认识到深度网络本身的作用是进行(基于梯度的)迭代优化来压缩、线性化和稀疏化数据，就可能很容易理解人工神经网络再过去十年发生的“进化”。特别的是，它有助于解释为什么只有少数人通过人工选择的过程出现在顶端，从一边的MLP到CNN再到ResNets再到Transformer。相比之下，随机搜索网络结构，如神经结构搜索(Zoph 和 Le，2017; Baker 等，2017)和 AutoML (Hutter 等，2019) ，没有导致任何对一般任务有效的网络架构。我们推测，成功的架构在模拟数据压缩迭代优化方案方面正变得越来越有效和灵活。除了前面提到的 ReduNet 和 ResNet/ResNeXt 之间的相似之处之外，我们在这里讨论一些更多的示例。</p><p>(下面用ResNeXT解释了上述提出的简约原则的合理性)</p><h2 id="如何学习-自洽的原则"><a href="#如何学习-自洽的原则" class="headerlink" title="如何学习: 自洽的原则"></a>如何学习: 自洽的原则</h2><p>单凭简约原则并不能保证模型能够从感知到的外部世界数据中获取所有重要信息。例如，通过最小化交叉熵，将每个类映射到一维的 one-hot 向量，可以被视为一种简约的形式。这可能会学习到一个好的分类器，但学习到的特征会崩溃为单例（singleton），被称为神经崩溃。这样学习到的特征将不再包含足够的信息来重新生成原始数据。如果特征空间维数过低，则学习的模型对数据拟合不足；如果过高，模型可能会过度拟合。</p><p>更一般地说，该研究认为感知不同于特定任务的执行，感知的目标是学习感知到的一切可预测的东西，即智能系统应该能够从压缩表示中重新生成观测数据的分布，使其达到自身内部无法区分的程度。为了控制学习完全忠于表征过程，该研究引入了第二个原则：<strong>自洽原则，即自主智能系统通过最小化被观测对象与重新生成对象之间的内部差异，来寻求外部世界观测的最自洽模型</strong>。</p><p>自洽和简约这两个原则是高度互补的，应该经常一起使用。自洽原则本身并不能保证压缩或效率方面的任何增益。从数学和计算上来说，将任何训练数据与过度参数化的模型拟合或通过在具有相同维度的域之间建立一对一映射来确保一致性，而不需要学习数据分布中的内在结构，这是容易的，甚至是微不足道的。只有通过压缩，才能迫使智能系统在高纬度感官数据中发现固有的低维结构，并用最紧凑的方式在特征空间中进行转换和表示，以备将来使用。此外，只有通过压缩，我们才能容易地理解为什么过度参数化，比如像 DNN 中通常做的那样用数百个通道提升特征，如果其纯粹的目的是在更高维度的特征空间中压缩，则不会导致过度拟合: 提升有助于减少数据中的非线性，因此使其更容易压缩和线性化。后续层的作用是执行压缩(和线性化)的 ，一般来说，层越多，压缩效果越好。</p><p>到目前为止，我们已经确定，我们需要一个机制，以确定是否压缩表示包含所有的信息是感知。在本节的剩余部分中，<strong>我们将首先介绍一个实现这个目标的通用架构——一个生成模型，它可以从压缩表示中重新生成样本。然后出现了一个困难的问题: 如何明智地测量被感测样品和再生样品之间的差异？我们认为，对于自治系统来说，解决这个问题只有一个办法，那就是在内部特征空间中测量它们之间的差异。最后，我们认为压缩编码器和生成器必须通过零和博弈一起学习。通过这些演绎，我们得出了一个普遍的学习框架，我们认为这是不可避免的。</strong></p><p><strong>具有可计算性的自动编码及其注意事项</strong></p><p>为了确保学习特征映射 f 和表示 z 能够正确地捕获数据中的低维结构，<strong>人们可以检查压缩特征 z 是否能够通过一些生成图 g 重现原始数据 x，这些图 g 参数化为 η: 在这里， x = g (z，η)接近 x 的意义上(根据某种度量)。</strong></p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 11.png)</p><p>这个过程通常被称为自动编码。在压缩到结构化表示(如 LDR)的特殊情况下，我们称这种自动编码为转录。然而，这个目标说起来容易做起来难。主要的困难在于如何使这个目标在计算上易于处理，从而在物理上可以实现。更准确地说，什么是既定义精确又可有效计算的原则性度量，用来衡量 x 分布和 x 分布之间的差异？正如我们之前提到的，当处理具有退化的低维支持的高维空间分布时(这在现实世界的数据中几乎总是如此)，传统的度量方法，如KL散度、互信息、Jensen-Shannon距离、Helmholtz自由能和Wasserstein距离可能是不明确的或难以计算的，即使是高斯(在子空间上支持)及其混合25。在比较高维空间中的退化分布时，我们如何解决这个基本的、但往往未被承认的可计算性方面的困难?</p><p><strong>用于自洽的闭环数据转录</strong></p><p>正如我们在上一节中所看到的，速率减小量∆R给出了简并分布之间定义良好的原则性距离度量。但它只对混合子空间或高斯分布可计算(封闭形式)，而不是一般的分布!然而，我们只能期望内部结构化表示z的分布是子空间或高斯分布的混合物，而不是原始数据x。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 12.png)</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 13.png)</p><p>这有效地导致了一个“闭环”反馈系统，整个过程如图6所示。编码器 f 现在扮演了一个额外的角色，作为一个鉴别器，通过它们的内部特征 z 和 z 之间的差异来检测 x 和 x 之间的任何差异。Z 分布与 z 分布之间的距离可以通过它们的样品 Z 和 Z 的速率折减(2)来测量:</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 14.png)</p><p>人们可以将学习DNN分类器f或生成器g的流行实践解释为单独学习闭环系统的一个开放段(图6)。这种目前流行的实践与开环控制非常相似，开环控制在控制社区中早就被认为是有问题的和昂贵的:训练这样的段需要对期望的输出进行监督(例如，类标签);而且，如果数据分布、系统参数或任务发生变化，这种开环系统的部署在本质上就不稳定、不健壮或不自适应。例如，在监督设置下训练的深度分类网络，如果用新的数据类别进行新任务的再训练，常常会遭受灾难性的遗忘(McCloskey和Cohen, 1989)。相比之下，闭环系统在本质上更稳定和自适应(Wiener, 1948)。事实上，Hinton et al.(1995)曾提出，在一个完整的学习过程中，判别和生成两个阶段需要分别组合为“觉醒”和“睡眠”阶段。</p><p><strong>通过自我批判进行自学习</strong></p><p>然而，仅仅关闭循环是不够的。现在我们只需要优化生成器 g，以便最小化 z 和 z 之间的差，这是很有诱惑力的想法，比如在速率降低措施方面</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 15.png)</p><p><strong>自洽增量式无监督学习</strong></p><p>到目前为止，我们主要讨论了监督设置中的两个原则。事实上，我们的框架的主要优势之一是，它是最自然和有效的自我学习，通过自我监督和自我批评。此外，由于速率降低寻求对学习的结构的显式(子空间类型)表示，29这使得在学习新任务/数据时，过去的知识很容易被保存，就像之前的(记忆)保持自洽一样。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 16.png)</p><p>最近的实证研究(Tong et al.， 2022)表明，这可能会导致第一个具有固定容量的独立神经系统，它可以逐步学习良好的LDR表征，而不会遭受灾难性的遗忘(McCloskey和Cohen, 1989)。</p><p>对于这样一个闭环系统，遗忘(如果有的话)是相当优雅的。此外，当一个老班级的图像再次被提供给系统来复习时，学习到的表现可以进一步巩固——这与人类记忆的特征非常相似。在某种意义上，这种受限的闭环公式本质上确保了视觉记忆的形成可以是贝叶斯和自适应的——假设大脑所希望的特征(Friston, 2009)。</p><p>请注意，该框架的基本构想是在完全无监督的设置中工作。因此，即使出于示范目的，该研究提出了假设类信息可用的原则，但该框架可以自然地扩展到完全无监督的设置，其中没有为任何数据样本提供类信息。在这种情况下，只需将每个新样本及其增强视为（15）中的一个新类。这可以被视为一种自监督。结合自批评游戏机制，可以轻松学习压缩闭环转录。</p><p>如图 8 所示，如此学习的自动编码不仅表现出良好的样本一致性，而且学习到的特征还表现出清晰且有意义的局部低维（薄）结构。更令人惊讶的是，即使在训练期间根本没有提供任何类信息，子空间或特征相关的块对角结构也开始出现在为类学习的特征中（图 9）。因此，学习到的特征结构类似于在灵长类动物大脑中观察到的类别选择区域。</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 17.png)</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 18.png)</p><h1 id="通用学习引擎"><a href="#通用学习引擎" class="headerlink" title="通用学习引擎"></a>通用学习引擎</h1><p>在以上章节中，我们以视觉影像资料建模为例，从简约与自洽的第一原则出发，推导出压缩闭环转录框架。在剩下的两个部分中，我们提供了更多关于这个框架的普遍性的思考，将其扩展到三维视觉和强化学习(本部分的其余部分)30，并预测其对神经科学、数学和更高层次智力的影响(第4部分)。</p><p><strong>“团结起来建设”和“分而治之”</strong></p><p>在压缩闭环转录框架中，我们已经看到了为什么以及如何从编码/信息理论、反馈控制、深度网络、优化和博弈论的基本思想和概念聚集在一起，成为一个完整的智能系统的组成部分，可以学习。虽然“分而治之”长期以来一直是科学研究中珍视的宗旨，但当涉及到理解智力这样的复杂系统时，相反的“团结和建设”应该是选择的宗旨。否则，我们将永远是盲人摸象的人:每个人总是认为一小块就是整个世界，并倾向于夸大它的重要性</p><p>这两个原则共同作为粘合剂，将许多必要的部分结合在一起，形成智力拼图，深层网络的作用自然而清楚地显示为外部观察和内部表征之间非线性映射的模型。有趣的是，这些原理揭示了学习系统的计算机制，这些机制类似于在大脑中观察到的或假设大小的一些关键特征，如稀疏编码和子空间编码(Barlow，1961; Olshausen and Field，1996; Chang 和 Tsao，2017) ，闭环反馈(Wiener，1948)和自由能最小化(Friston，2009) ，我们将在下一节讨论更多。</p><p>闭环压缩结构在自然界中无处不在，适用于所有智能生物，这一点可以见于大脑（压缩感觉信息）、脊髓回路（压缩肌肉运动）、DNA（压缩蛋白质的功能信息）等等生物示例。因此，他们认为，压缩闭环转录可能是所有智能行为背后的通用学习引擎。它使智能生物和系统能够从看似复杂和无组织的输入中发现和提炼低维结构，并将它们转换为紧凑和有组织的内部结构，以便记忆和利用。</p><p>为了说明这种框架的普遍性，对于本节的其余部分，我们将研究另外两个任务: 三维感知和决策，这被认为是任何自主智能系统的两个关键模块(LeVillage，2022)。我们推测如何在这两个原则的指导下，一个人可以发展不同的观点和新的见解来理解这些具有挑战性的任务。</p><h2 id="3D-感知-视觉和图形的闭环回路"><a href="#3D-感知-视觉和图形的闭环回路" class="headerlink" title="3D 感知: 视觉和图形的闭环回路"></a>3D 感知: 视觉和图形的闭环回路</h2><p>到目前为止，我们已经证明成功的闭环转录发现紧凑结构的数据集的二维图像。这依赖于每个类别的图像数据之间存在统计相关性。我们相信，如果数据中的低维结构是通过硬物理或几何约束而不是软统计相关性来定义的，相同的压缩机制将会更加有效。</p><p>David Marr 在其颇具影响力的著作《视觉》一书中提出的 3D 视觉经典范式提倡“分而治之”的方法，将 3D 感知任务划分为几个模块化过程：从低级 2D 处理（如边缘检测、轮廓草图）、中级 2.5D 解析（如分组、分割、图形和地面），以及高级 3D 重建（如姿势、形状）和识别（如对象），而相反，压缩闭环转录框架提倡“联合构建”思想。</p><p><strong>感知是压缩闭环转录？</strong></p><p>更准确地说，世界上物体的形状、外观甚至动态的 3D 表示应该是我们的大脑内部开发的最紧凑和结构化的表示，以相应地解释所有感知到的视觉观察。如果是这样，那么这两个原理表明紧凑和结构化的 3D 表示就是要寻找的内部模型。这意味着我们可以并且应该在一个闭环计算框架内统一计算机视觉和计算机图形，如下图所示：</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 19.png)</p><p>计算机视觉通常被解释为为所有 2D 视觉输入重建和识别内部 3D 模型的前向过程，而计算机图形学表示其对内部 3D 模型进行渲染和动画处理的逆过程。将这两个过程直接组合成一个闭环系统可能会带来巨大的计算和实践好处：几何形状、视觉外观和动力学中的所有丰富结构（例如稀疏性和平滑度）可以一起用于统一的 3D 模型， 最紧凑，且与所有视觉输入一致。</p><p>计算机视觉中的识别技术可以帮助计算机图形学在形状和外观空间中构建紧凑模型，并为创建逼真的 3D 内容提供新的方法。另一方面，计算机图形学中的 3D 建模和仿真技术可以预测、学习和验证计算机视觉算法分析的真实对象和场景的属性和行为。视觉和图形社区长期以来一直在实践“综合分析”的方法。</p><p><strong>外观和形状的统一表示？</strong></p><p>基于图像的渲染，其中，通过从一组给定图像中学习来生成新视图，可以被视为早期尝试用简约和自洽的原理缩小视觉和图形之间的差距。特别是，全光采样表明，可以用所需的最少图像数量（简约性）来实现抗锯齿图像（自洽性）。</p><p>最近在辐射场建模方面的发展为这一观点提供了更多的经验证明(Yu et al。 ，2021) : 直接利用辐射场中的低维结构的三维(稀疏支持和空间平滑性)比黑盒子深度神经网络的蛮力训练更有效率和有效的解决方案(Mildenhall et al。 ，2020)。然而，对于未来来说，确定合适的紧凑和结构化三维表示家族仍然是一个挑战，这些三维表示可以在统一的框架中集成形状几何，外观，甚至动态，从而使数据，模型和计算的复杂性最小</p><h2 id="决策-关闭感知、学习和行动的循环"><a href="#决策-关闭感知、学习和行动的循环" class="headerlink" title="决策: 关闭感知、学习和行动的循环"></a>决策: 关闭感知、学习和行动的循环</h2><p>到目前为止，我们已经讨论了如何压缩闭环转录可能导致一个有效和高效的框架学习一个良好的知觉模型从视觉输入。在下一个层次上，这样的感知模型可以被一个自主智能体用来在复杂的动态环境中完成特定的任务。代理人从感知的结果中学习或从其行动中获得奖励的整个过程在更高的层次上形成了另一个封闭的循环(图11)。闭环反馈系统的作用是确保主体的学习模型和控制策略与外部世界一致，使模型能够对状态(st)转移做出最佳预测，而行为(at)的学习控制策略 πθ 导致最大期望报酬 R:</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 20.png)</p><p>注意这里的奖励 R 起着类似于 LDR 模型的降息目标(4)的作用，它测量学习控制策略 π 的“优点”并指导其改进。节俭的原则是现代强化学习在处理大规模任务如 Alpha-Go (Silver et al。 ，2016,2017)和玩视频游戏(Berner et al。 ，2019; Vinyals et al。 ，2019)方面非常成功的主要原因。在几乎所有具有天文大小或维度的状态作用空间的任务中，比如说 D，实践者总是假设最优值函数 V * ，Q-函数 Q * ，或者策略 π * 只取决于一小部分，比如说 d D，特征:</p><p>![Untitled](E:\desktop\Export-61c461d9-5959-4eef-91e2-65461099398c\On the Principles of Parsimony and Self-Consistenc 571c8ea2887146eab8f8436deccda2de\Untitled 21.png)</p><p>其中 f (s，a)∈ Rd 是一个非线性映射，它学习极大或高维状态作用空间的一些低维特征。在视频游戏中，状态维度 D 很容易达到数百万，然而学习一个好的策略所需要的特性数量通常只有几十个或几百个！通常，OC/rL 中寻求的这些最优控制策略或价值/回报函数甚至被假定为这些特征的线性叠加(Ng 和 Russell，2000; Kakade，2001) : ω &gt; f (s，a) = ω1 · f1(s，a) + … + ωd · fd (s，a)。(18)假设非线性映射 f 也能线性化策略/值/报酬函数对学习特征的依赖关系</p><p><strong>通过游戏自主选择特征？</strong></p><p>请注意，RL 中的所有这些实践在精神上都非常类似于2.1节所述的节约原则下的学习目标。有效地利用低维结构是学习能够在如此高维的状态作用空间中如此可扩展的(唯一)原因; 正确识别和线性化这样的低维结构是所学习的控制策略可推广的关键33。然而，在实践中，人们对特征 d 的选择仍然是启发式的。这使得整个 RL 不是自治的。我们相信，对于一个闭环学习系统来自动确定与奖励/任务相关的特征的正确数量，人们必须将 RL (16)的公式扩展到某个最大值游戏34，以类似于第2.2节中研究的精神实现视觉建模的自我一致性。</p><p><strong>RL的数据和计算效率?</strong></p><p>近年来，已有许多理论尝试从马尔科夫决策过程(MDP)的采样和计算复杂度的角度来解释经验观察到的强化学习效率。然而，任何基于非结构化通用mdp和奖励函数的理论都不能为这些经验上的成功提供相关的解释。例如，强化学习的样本复杂性的一些最著名的边界在状态空间和动作O的基数上保持线性(|S||A|) (Li et al.， 2020)，这不能解释在状态或动作空间是天文数字的大规模任务(如alphago和视频游戏)中经验观察到的RL效率。我们认为RL在处理许多实际的大规模任务时的效率只能来自于系统动力学中固有的低维性或最优策略/控制与状态之间的相关性。例如，假设系统具有有界的逃避维度(Osband和Roy, 2014)或MDPs是低秩的(Uehara et al.， 2021;Agarwal等人，2020年)。深度网络的作用再次是识别和建模这种低维结构，并希望将其线性化。</p><p>总之，对于大规模的RL任务，正是这两个原则使这样一个感知、学习和行动的闭环系统成为真正高效的学习引擎。有了这样的引擎，自主代理就能够发现环境和学习任务中确实存在低维结构，并最终在学习到的结构足够好并能很好地概括时智能地行动!</p><h1 id="更广泛的智能"><a href="#更广泛的智能" class="headerlink" title="更广泛的智能"></a><strong>更广泛的智能</strong></h1><h2 id="智能的神经科学"><a href="#智能的神经科学" class="headerlink" title="智能的神经科学"></a><strong>智能的神经科学</strong></h2><p>人们会期望基本的智能原理对大脑的设计产生重大影响。简约和自洽原理为灵长类视觉系统的几个实验观察提供了新的思路。更重要的是，它们揭示了未来实验中要寻找的目标。<br>作者团队已经证明，仅寻求内部简约和预测性表示就足以实现“自监督”，允许结构自动出现在通过压缩闭环转录学习的最终表示中。<br>例如，图 9 显示无监督数据转录学习自动区分不同类别的特征，为在大脑中观察到的类别选择性表示提供了解释。这些特征也为灵长类大脑中稀疏编码和子空间编码的广泛观察提供了合理的解释。此外，除了视觉数据建模，最近的神经科学研究表明，大脑中出现的其他结构化表示（例如“位置细胞”）也可能是以最压缩的方式编码空间信息的结果。<br>可以说，最大编码率降低 (MCR2) 原理在精神上类似于认知科学中的“自由能最小化原理”（free energy minimization principle），后者试图通过能量最小化为贝叶斯推理提供框架。但与自由能的一般概念不同，速率降低在计算上易于处理且可直接优化，因为它可以以封闭的形式表示。此外，这两个原理的相互作用表明，正确模型（类）的自主学习应该通过对这种效用的闭环最大化博弈来完成，而不是单独进行最小化。因此，他们相信，压缩闭环转录框架为如何实际实施贝叶斯推理提供了一个新的视角。<br>这个框架也被他们认为阐明了大脑使用的整体学习架构，可以通过展开优化方案来构建前馈段，且不需要通过反向传播从随机网络中学习。此外，框架存在一个互补的生成部分，可以形成一个闭环反馈系统来指导学习。<br>最后，框架揭示了许多对“预测编码”大脑机制感兴趣的神经科学家所寻求的难以捉摸的“预测错误”信号，这是一种与压缩闭环转录产生共振的计算方案：为了让计算更容易，应在表示的最后阶段测量传入和生成的观测值之间的差异。</p><h2 id="迈向更高层次的智能"><a href="#迈向更高层次的智能" class="headerlink" title="迈向更高层次的智能"></a><strong>迈向更高层次的智能</strong></h2><p>马毅等人的工作认为，压缩闭环转录与Hinton等人在1995年提出的框架相比，在计算上更易于处理和可扩展。而且，循环的学习非线性编码/解码映射（通常表现为深度网络），本质上在外部无组织的原始感官数据（如视觉、听觉等）和内部紧凑和结构化表示之间提供了一个重要的“接口”。<br>不过，他们也指出，这两个原理并不一定能解释智能的所有方面。高级语义、符号或逻辑推理的出现和发展背后的计算机制仍然难以捉摸。直到今天，关于这种高级符号智能是可以从持续学习中产生还是必须进行硬编码，仍然存在争议。<br>在三位科学家看来，诸如子空间之类的结构化内部表示是高级语义或符号概念出现的必要中间步骤——每个子空间对应一个离散的（对象）类别。如此抽象的离散概念之间的其他统计、因果或逻辑关系可以进一步简化建模为紧凑和结构化（比如稀疏）图，每个节点代表一个子空间/类别。可以通过自动编码来学习图形以确保自一致性。<br>他们推测，只有在个体智能体学习的紧凑和结构化表示之上，高级智能（具有可共享的符号知识）的出现和发展才有可能。因此，他们建议，应该通过智能系统之间有效的信息交流或知识迁移来探索高级智能出现的新原理（如果高级智能存在的话）。<br>此外，更高级别的智能应该与我们在本文中提出的两个原理有两个共同点：<br>可解释性：所有原理都应该有助于将智能的计算机制揭示为白盒，包括可测量的目标、相关的计算架构和学习表示的结构。<br>可计算性：任何新的智能原理都必须在计算上易于处理和可扩展，可以通过计算机或自然物理实现，并最终得到科学证据的证实。<br>只有具备可解释和可计算性，我们才能无需依赖当前昂贵且耗时的“试错”方法来推进人工智能的进步，能够描述完成这些任务所需的最少数据和计算资源，而不是简单地提倡“越大越好”的蛮力方法。智慧不应该是最足智多谋的人的特权，在一套正确的原则下，任何人都应该能够设计和构建下一代智能系统，无论大小，其自主性、能力和效率最终都可以模仿甚至超过动物和人类。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>通过这篇文章，我们希望能让读者相信，在揭示、理解甚至利用智慧的作品方面，我们现在所处的位置要比七十年前的维纳和香农等人好得多。我们已经提出并论证，在“简约”(Parsimony)和“自我一致性”(Self-consistency)这两个原则下，我们有可能将许多智能拼图的必要碎片组装成一个统一的计算框架，这个框架很容易在机器上或本质上实现。这个统一的框架为我们如何进一步推进感知、决策和智力的研究提供了新的视角。</p><p>为了结束我们对情报工作的原则性方法的建议，我们再次强调，所有情报工作的科学原则不应该仅仅是哲学指导方针或仅仅是由难以计算或只能通过启发式近似的数学/统计量制定的概念框架。它们应该依赖于最基本和原则性的目标，这些目标可以用有限的观测来衡量，并导致即使使用有限的资源也可以实现的计算系统。开尔文勋爵(Lord Kelvin)的一句话或许最能表达这种信念:38岁</p><p>“当你能够衡量你所说的内容并用数字表达出来时，你就知道了一些;但是，当你不能衡量它，当你不能用数字来表达它时，你的知识就是贫乏的，不能令人满意的。它可能是知识的开端，但在你的思想中，无论情况如何，你几乎还没有达到科学的阶段。”</p><p>——开尔文勋爵(1883年</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-7-12 日记</title>
      <link href="/post/49295.html"/>
      <url>/post/49295.html</url>
      
        <content type="html"><![CDATA[<h1 id="这是一篇日记"><a href="#这是一篇日记" class="headerlink" title="这是一篇日记"></a>这是一篇日记</h1><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><p>从7月5日完成整个博客框架到现在，除了帮研究生那边弄了点数据，基本没做什么事情，整个人处于摆烂的状态，感觉对什么事情都提不起兴趣，偶尔想要做点事情，但是又没有目标，不知道干什么，于是就通过不停游戏和刷知乎来麻痹自己。今天回想起来有一种深深的无力感，事情发展到现在对研究生生涯充满了悲观，也没有任何有效的方法和手段能让自己摆脱目前的困境。总结来说面临的问题有下面几点：</p><ol><li>研究生那边没有进行任何有效地科研相关的指导，目前虽然有了课题但处于完全不了解的状态，负责人也只是让我处理一些无关紧要的东西，对后续发展几无裨益</li><li>对后续发展没有任何清晰有效的规划，不知道读研之后要干什么，读博？工作？考公？似乎都可以，又似乎都难以令人满意，再加上第一点的影响，让我原本踌躇满志的想要在研究生做出成绩的心理几乎消失殆尽，但是想到浑浑噩噩地混到毕业，最后工作不能达到预期，又会有一种迫切地想做点儿事情的焦虑，偏偏靠自己什么也不会做</li><li>对待钱的态度问题，对自己已经大学毕业了但没有任何途径能够赚钱，只能啃老，让自己内心非常愧疚+焦虑，偏偏还因为研究生那边的人际问题导致失去了实习的机会，现在特别后悔</li><li>研究生一坨屎一样的人际关系，A似乎画大饼严重，从去年九月份至今几乎没有提供任何实质性的科研相关的指导，问就是等两天，等着等着，我从去年保研结束等到了现在，几乎一点儿有用的没学。虽然聊天时感觉为人并不高傲，但到目前为止几乎让我对后续跟着A做东西的心情和动力完全消失，而且该方向过于冷门，在工业界没有落地应用，而且发论文的难度也远远高于其他方向。目前已经完全不想主动要求做事了。B已经完全失联，完全不会看消息，回复消息，虽然为人看起来是踌躇满志，很有野心，但是整体完全不靠谱，后续如果真的要跟随B进行研究对心态完全是折磨。而且AB之间似乎并没有很融洽，老师和稀泥般让我两边都跟着，目前来看B完全没有指导的意思。</li></ol><h2 id="后续计划"><a href="#后续计划" class="headerlink" title="后续计划"></a>后续计划</h2><p>现在脑袋里一脑袋浆糊，一边焦虑自己摆烂导致难以毕业，另一边又不知道该干什么。现在能想到的就是整理串联论文，按照写一篇综述论文的思路去收集，阅读一部分论文，自己写一篇小综述，了解后续要研究的题目。暂定。</p>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建日记</title>
      <link href="/post/44750.html"/>
      <url>/post/44750.html</url>
      
        <content type="html"><![CDATA[<h2 id="2022年7月2日"><a href="#2022年7月2日" class="headerlink" title="2022年7月2日"></a>2022年7月2日</h2><p>截止7月2日，博客的基本框架已经完成，目前的功能包括：</p><ol><li>主题美化</li><li>可能用到的框架页面如迎接语，标签，目录，友链，个人链接，valine评论区构建</li><li>更新推送日历</li><li>看板娘，已经改为卡通狗的形象</li></ol><p>目前存在的问题:</p><ol><li>评论区没有实时邮件推送，评论区头像很丑</li><li>音乐界面不能播放，跳转后音乐中断</li><li>豆瓣无法爬取图书，电影</li><li>背景未透明</li></ol><h2 id="2022年7月3日"><a href="#2022年7月3日" class="headerlink" title="2022年7月3日"></a>2022年7月3日</h2><p>今天的更新内容：</p><ol><li>增加TwiKoo评论区，这可比valine好使多了，这告诉我们，选择比努力更重要o(￣ヘ￣o＃)</li><li>添加进入荧光效果</li><li>音乐界面BUG已经修复</li><li>增加悬赏</li><li>banner透明，foot透明</li></ol><p>存在问题：</p><ol><li>音乐底部吸附播放器不好使</li><li>豆瓣未修复</li><li>照片墙未更新</li><li>留言板后端未配置</li></ol><h2 id="2022年7月4日"><a href="#2022年7月4日" class="headerlink" title="2022年7月4日"></a>2022年7月4日</h2><p>今日更新内容：</p><ol><li>音乐功能完成，但是VIP歌曲不能放</li><li>本地搜索功能</li><li>照片墙根本找不到相关插件，算了，累了，毁灭吧</li><li>留言板就用评论好了，反正有邮件通知功能</li><li>加了小人转圈小挂件</li><li>小闹钟，但是这个闹钟插件本身存在bug，只能根据移动网络的IP地址进行定位，浪费了大量时间，[○･｀Д´･ ○]</li><li>豆瓣修复了（NND，自己写什么写，找个插件直接用不香么，(⊙o⊙）无语了）</li><li>在foot上加个小心心</li><li>解决了文章封面问题，文章顶部图片问题，添加了百度图片爬虫以便随时进行图片获取</li></ol><h2 id="小小的总结一下"><a href="#小小的总结一下" class="headerlink" title="小小的总结一下"></a>小小的总结一下</h2><p>到此为止，博客大致已经部署完成了，后续可能还会添加一些小的功能，大的内容基本不会变了，搭了五天，第一次纯纯地用前端的工具做项目，踩了无数的坑。好在磕磕绊绊地终于整出来了，( Ĭ ^ Ĭ )。</p><p>很久之前就想做个自己的博客了，有没有人看倒是无所谓，主要是记录加深自己对知识的理解。但是作为重度拖延症患者，就愣是拖到现在了。以前在CSDN这类博客里面写，确实很方便，只需要打字就好了，但是毕竟不是自己做的东西，换个背景都得掏钱，哼，臭资本家，我是你工人爷爷，我分享点儿东西还得给你钱？用我的知识，以你的名义，我呸！</p><p>总之是这几天趁着没事儿写了一下，对于拖延症的问题，我反思，我悔过，但是下次还敢。</p><p>这几天的经历也让我学到了很多东西，首先是前端的内容没有想象的简单，以前觉得做前端，随便找个模板改改就好了，这次是尝到了从无到有的写前端的艰辛了，而且前端往往跟后端也难分难割，两个的交互写得真的累，而且前端的东西真的好多，不仅仅就HTML，CSS，JS就完事了，向广大前端设计师道歉（┭┮﹏┭┮）！此外，我觉得能用代码做点儿自己喜欢的事情是真的会觉得开心并且获得成就感的，比起以前被迫做东西，被迫学东西，做自己喜欢的事情真好。</p><p>OK，以后这里不仅会作为我个人的技术分享博客，也会作为我的学习内容笔记以及我的部分日记分享博客，好了，散会！</p>]]></content>
      
      
      <categories>
          
          <category> 博客搭建日记 </category>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
            <tag> Hexo </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>测试测试</title>
      <link href="/post/63786.html"/>
      <url>/post/63786.html</url>
      
        <content type="html"><![CDATA[<p>这是第一篇测试博客，查看网站对md文件的支持是否符合要求</p><p>[TOC]</p><h2 id="标签测试"><a href="#标签测试" class="headerlink" title="标签测试"></a>标签测试</h2><h3 id="样式一"><a href="#样式一" class="headerlink" title="样式一"></a>样式一</h3><h4 id="simple-样式"><a href="#simple-样式" class="headerlink" title="simple 样式"></a>simple 样式</h4><div class="note simple"><p>默认 提示块标签</p></div><div class="note default simple"><p>default 提示块标签</p></div><div class="note primary simple"><p>primary 提示块标签</p></div><div class="note success simple"><p>success 提示块标签</p></div><div class="note info simple"><p>info 提示块标签</p></div><div class="note warning simple"><p>warning 提示块标签</p></div><div class="note danger simple"><p>danger 提示块标签</p></div> <h4 id="modern样式"><a href="#modern样式" class="headerlink" title="modern样式"></a>modern样式</h4><div class="note modern"><p>默认 提示块标签</p></div><div class="note default modern"><p>default 提示块标签</p></div><div class="note primary modern"><p>primary 提示块标签</p></div><div class="note success modern"><p>success 提示块标签</p></div><div class="note info modern"><p>info 提示块标签</p></div><div class="note warning modern"><p>warning 提示块标签</p></div><div class="note danger modern"><p>danger 提示块标签</p></div> <h4 id="flat样式"><a href="#flat样式" class="headerlink" title="flat样式"></a>flat样式</h4><div class="note flat"><p>默认 提示块标签</p></div><div class="note default flat"><p>default 提示块标签</p></div><div class="note primary flat"><p>primary 提示块标签</p></div><div class="note success flat"><p>success 提示块标签</p></div><div class="note info flat"><p>info 提示块标签</p></div><div class="note warning flat"><p>warning 提示块标签</p></div><div class="note danger flat"><p>danger 提示块标签</p></div> <h4 id="disable样式"><a href="#disable样式" class="headerlink" title="disable样式"></a>disable样式</h4><div class="note disabled"><p>默认 提示块标签</p></div><div class="note default disabled"><p>default 提示块标签</p></div><div class="note primary disabled"><p>primary 提示块标签</p></div><div class="note success disabled"><p>success 提示块标签</p></div><div class="note info disabled"><p>info 提示块标签</p></div><div class="note warning disabled"><p>warning 提示块标签</p></div><div class="note danger disabled"><p>danger 提示块标签</p></div><h4 id="no-icon样式"><a href="#no-icon样式" class="headerlink" title="no-icon样式"></a>no-icon样式</h4><div class="note no-icon flat"><p>默认 提示块标签</p></div><div class="note default no-icon flat"><p>default 提示块标签</p></div><div class="note primary no-icon flat"><p>primary 提示块标签</p></div><div class="note success no-icon flat"><p>success 提示块标签</p></div><div class="note info no-icon flat"><p>info 提示块标签</p></div><div class="note warning no-icon flat"><p>warning 提示块标签</p></div><div class="note danger no-icon flat"><p>danger 提示块标签</p></div><h3 id="样式二"><a href="#样式二" class="headerlink" title="样式二"></a>样式二</h3><h4 id="simple样式"><a href="#simple样式" class="headerlink" title="simple样式"></a>simple样式</h4><div class="note icon-padding simple"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding simple"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding simple"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding simple"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding simple"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding simple"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding simple"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div> <h4 id="modern样式-1"><a href="#modern样式-1" class="headerlink" title="modern样式"></a>modern样式</h4><div class="note icon-padding modern"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding modern"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding modern"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding modern"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding modern"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding modern"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><h4 id="flat样式-1"><a href="#flat样式-1" class="headerlink" title="flat样式"></a>flat样式</h4><div class="note icon-padding flat"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding flat"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding flat"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding flat"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding flat"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding flat"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><h4 id="disable样式-1"><a href="#disable样式-1" class="headerlink" title="disable样式"></a>disable样式</h4><div class="note icon-padding disabled"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding disabled"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding disabled"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding disabled"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding disabled"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding disabled"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding disabled"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><h4 id="no-icon样式-1"><a href="#no-icon样式-1" class="headerlink" title="no-icon样式"></a>no-icon样式</h4><div class="note no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue no-icon flat"><p>2021年快到了….</p></div><div class="note pink no-icon flat"><p>小心开车 安全至上</p></div><div class="note red no-icon flat"><p>这是三片呢？还是四片？</p></div><div class="note orange no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple no-icon flat"><p>剪刀石头布</p></div><div class="note green no-icon flat"><p>前端最讨厌的浏览器</p></div><h2 id="上标标签tip测试"><a href="#上标标签tip测试" class="headerlink" title="上标标签tip测试"></a>上标标签tip测试</h2><div class="tip "><p>默认情况</p></div><div class="tip success"><p>success</p></div><div class="tip error"><p>error</p></div><div class="tip warning"><p>warning</p></div><div class="tip bolt"><p>bolt</p></div><div class="tip ban"><p>ban</p></div><div class="tip home"><p>home</p></div><div class="tip sync"><p>sync</p></div><div class="tip cogs"><p>cogs</p></div><div class="tip key"><p>key</p></div><div class="tip bell"><p>bell</p></div><div class="tip fa-atom"><p>自定义font awesome图标</p></div><h2 id="动态标签anima测试"><a href="#动态标签anima测试" class="headerlink" title="动态标签anima测试"></a>动态标签anima测试</h2><p>On DOM load（当页面加载时显示动画）</p><div class="tip warning faa-horizontal animated"><p>warning</p></div><div class="tip ban faa-flash animated"><p>ban</p></div><p>调整动画速度</p><div class="tip warning faa-horizontal animated faa-fast"><p>warning</p></div><div class="tip ban faa-flash animated faa-slow"><p>ban</p></div><p>On hover（当鼠标悬停时显示动画）</p><div class="tip warning faa-horizontal animated-hover"><p>warning</p></div><div class="tip ban faa-flash animated-hover"><p>ban</p></div><p>On parent hover（当鼠标悬停在父级元素时显示动画）</p><div class="tip warning faa-parent animated-hover"><p class="faa-horizontal">warning</p></div><div class="tip ban faa-parent animated-hover"><p class="faa-flash">ban</p></div><h2 id="复选列表checkbox测试"><a href="#复选列表checkbox测试" class="headerlink" title="复选列表checkbox测试"></a>复选列表checkbox测试</h2><div class="checkbox"><input type="checkbox">            <p>纯文本测试</p>            </div><div class="checkbox checked"><input type="checkbox" checked="checked">            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class="checkbox red"><input type="checkbox">            <p>支持自定义颜色</p>            </div><div class="checkbox green checked"><input type="checkbox" checked="checked">            <p>绿色 + 默认选中</p>            </div><div class="checkbox yellow checked"><input type="checkbox" checked="checked">            <p>黄色 + 默认选中</p>            </div><div class="checkbox cyan checked"><input type="checkbox" checked="checked">            <p>青色 + 默认选中</p>            </div><div class="checkbox blue checked"><input type="checkbox" checked="checked">            <p>蓝色 + 默认选中</p>            </div><div class="checkbox plus green checked"><input type="checkbox" checked="checked">            <p>增加</p>            </div><div class="checkbox minus yellow checked"><input type="checkbox" checked="checked">            <p>减少</p>            </div><div class="checkbox times red checked"><input type="checkbox" checked="checked">            <p>叉</p>            </div><h2 id="单选列表radio"><a href="#单选列表radio" class="headerlink" title="单选列表radio"></a>单选列表radio</h2><div class="checkbox"><input type="radio">            <p>纯文本测试</p>            </div><div class="checkbox checked"><input type="radio" checked="checked">            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class="checkbox red"><input type="radio">            <p>支持自定义颜色</p>            </div><div class="checkbox green"><input type="radio">            <p>绿色</p>            </div><div class="checkbox yellow"><input type="radio">            <p>黄色</p>            </div><div class="checkbox cyan"><input type="radio">            <p>青色</p>            </div><div class="checkbox blue"><input type="radio">            <p>蓝色</p>            </div><h2 id="时间轴timeline"><a href="#时间轴timeline" class="headerlink" title="时间轴timeline"></a>时间轴timeline</h2><div class="timeline undefined"><div class="timeline-item headline"><div class="timeline-item-title"><div class="item-circle"><p>时间线标题（可选）</p></div></div></div></div><h2 id="卡片链接"><a href="#卡片链接" class="headerlink" title="卡片链接"></a>卡片链接</h2><div class="tag link"><a class="link-card" title="但为君故の博客" href="https://tzy1997.com"><div class="left"><img src="https://bu.dusays.com/2022/01/14/21dcbc47444ab.jpg"></div><div class="right"><p class="text">但为君故の博客</p><p class="url">https://tzy1997.com</p></div></a></div><h2 id="按钮测试"><a href="#按钮测试" class="headerlink" title="按钮测试"></a>按钮测试</h2><h3 id="一"><a href="#一" class="headerlink" title="一"></a>一</h3><div class="btns circle grid5">            <a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg">xaoxuu</a><a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg">xaoxuu</a><a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg">xaoxuu</a><a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg">xaoxuu</a><a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg">xaoxuu</a>          </div><h3 id="二"><a href="#二" class="headerlink" title="二"></a>二</h3><div class="btns rounded grid5">            <a class="button" href="/" title="下载源码"><i class="fas fa-download"></i>下载源码</a><a class="button" href="/" title="查看文档"><i class="fas fa-book-open"></i>查看文档</a>          </div><h3 id="三"><a href="#三" class="headerlink" title="三"></a>三</h3><div class="btns circle center grid5">            <a href="https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1">  <i class="fab fa-apple"></i>  <b>心率管家</b>  <p class="p red">专业版</p>  <img src="https://bu.dusays.com/2022/05/19/6285336eb791e.png"></a><a href="https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1">  <i class="fab fa-apple"></i>  <b>心率管家</b>  <p class="p green">免费版</p>  <img src="https://bu.dusays.com/2022/05/19/62853399bd275.png"></a>          </div><h2 id="网站卡片tips"><a href="#网站卡片tips" class="headerlink" title="网站卡片tips"></a>网站卡片tips</h2><div class="site-card-group"><a class="site-card" href="https://xaoxuu.com"><div class="img"><img src="https://i.loli.net/2020/08/21/VuSwWZ1xAeUHEBC.jpg"></div><div class="info"><img src="https://bu.dusays.com/2022/05/02/626f92e193879.jpg"><span class="title">xaoxuu</span><span class="desc">简约风格</span></div></a><a class="site-card" href="https://inkss.cn"><div class="img"><img src="https://i.loli.net/2020/08/21/Vzbu3i8fXs6Nh5Y.jpg"></div><div class="info"><img src="https://inkss.cn/img/avatar.jpg"><span class="title">inkss</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://blog.mhuig.top"><div class="img"><img src="https://i.loli.net/2020/08/22/d24zpPlhLYWX6D1.png"></div><div class="info"><img src="https://static.mhuig.top/npm/mhg@0.0.0/avatar/avatar.png"><span class="title">MHuiG</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://colsrch.top"><div class="img"><img src="https://i.loli.net/2020/08/22/dFRWXm52OVu8qfK.png"></div><div class="info"><img src="https://avatars.githubusercontent.com/u/58458181?v=4"><span class="title">Colsrch</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://linhk1606.github.io"><div class="img"><img src="https://i.loli.net/2020/08/21/3PmGLCKicnfow1x.png"></div><div class="info"><img src="https://i.loli.net/2020/02/09/PN7I5RJfFtA93r2.png"><span class="title">Linhk1606</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a></div><h2 id="行内图片测试"><a href="#行内图片测试" class="headerlink" title="行内图片测试"></a>行内图片测试</h2><p>这是 <img no-lazy="" class="inline" src="https://bu.dusays.com/2022/05/19/628532706842d.gif" style="height:1.5em"> 一段话。</p><p>这又是 <img no-lazy="" class="inline" src="https://bu.dusays.com/2022/05/19/6285328a83ca7.gif" style="height:40px;"> 一段话。</p><h2 id="单张图片"><a href="#单张图片" class="headerlink" title="单张图片"></a>单张图片</h2><h3 id="一-普通"><a href="#一-普通" class="headerlink" title="一 普通"></a>一 普通</h3><div class="img-wrap"><div class="img-bg"><img class="img" src="https://bu.dusays.com/2022/05/19/6285306c996c4.jpg" alt="愿你成为自己的太阳，无需凭借谁的光芒。"></div><span class="image-caption">愿你成为自己的太阳，无需凭借谁的光芒。</span></div><h3 id="二-改变宽度"><a href="#二-改变宽度" class="headerlink" title="二 改变宽度"></a>二 改变宽度</h3><div class="img-wrap"><div class="img-bg"><img class="img" src="https://bu.dusays.com/2022/05/19/6285306c996c4.jpg" style="width:400px;"></div></div><h3 id="三-添加描述"><a href="#三-添加描述" class="headerlink" title="三 添加描述"></a>三 添加描述</h3><div class="img-wrap"><div class="img-bg"><img class="img" src="https://bu.dusays.com/2022/05/19/6285306c996c4.jpg" alt="愿你成为自己的太阳，无需凭借谁的光芒。" style="width:400px;"></div><span class="image-caption">愿你成为自己的太阳，无需凭借谁的光芒。</span></div><h3 id="四-占位背景色"><a href="#四-占位背景色" class="headerlink" title="四 占位背景色"></a>四 占位背景色</h3><div class="img-wrap"><div class="img-bg" style="background:#1D0C04"><img class="img" src="https://bu.dusays.com/2022/05/19/6285306c996c4.jpg" alt="优化不同宽度浏览的观感" style="width:400px;"></div><span class="image-caption">优化不同宽度浏览的观感</span></div><h2 id="音频测试"><a href="#音频测试" class="headerlink" title="音频测试"></a>音频测试</h2><div class="audio"><audio controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/Lumia1020.mp3" type="audio/mp3">Your browser does not support the audio tag.</audio></div><h2 id="视频测试"><a href="#视频测试" class="headerlink" title="视频测试"></a>视频测试</h2><p>一 全宽度</p><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div>二 半宽度<div class="videos" col="2"><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div></div><p>三 四分之一宽</p><div class="videos" col="4"><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div><div class="video"><video controls="" preload=""><source src="https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov" type="video/mp4">Your browser does not support the video tag.</video></div></div><h2 id="折叠款测试"><a href="#折叠款测试" class="headerlink" title="折叠款测试"></a>折叠款测试</h2><details><summary> 查看图片测试 </summary>              <div class="content">              <p><img src="https://bu.dusays.com/2022/05/19/628533399e7a1.jpg"></p>              </div>            </details><details cyan="" open=""><summary> 查看默认打开的折叠框 </summary>              <div class="content">              <p>这是一个默认打开的折叠框。</p>              </div>            </details><details green=""><summary> 查看代码测试 </summary>              <div class="content">              <p>假装这里有代码块（代码块没法嵌套代码块）</p>              </div>            </details><details yellow=""><summary> 查看列表测试 </summary>              <div class="content">              <ul><li>haha</li><li>hehe</li></ul>              </div>            </details><details red=""><summary> 查看嵌套测试 </summary>              <div class="content">              <details blue=""><summary> 查看嵌套测试2 </summary>              <div class="content">              <details><summary> 查看嵌套测试3 </summary>              <div class="content">              <p>hahaha <span><img src="https://bu.dusays.com/2022/05/19/62853244cef33.png" style="height:24px"></span></p>              </div>            </details>              </div>            </details>              </div>            </details><h2 id="分栏tab"><a href="#分栏tab" class="headerlink" title="分栏tab"></a>分栏tab</h2><p>Demo 1 - 预设选择第一个【默认】</p><div class="tabs" id="test1"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test1-1">test1 1</button></li><li class="tab"><button type="button" data-href="#test1-2">test1 2</button></li><li class="tab"><button type="button" data-href="#test1-3">test1 3</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>Demo 2 - 预设选择tabs</p><div class="tabs" id="test2"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test2-1">test2 1</button></li><li class="tab"><button type="button" data-href="#test2-2">test2 2</button></li><li class="tab active"><button type="button" data-href="#test2-3">test2 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test2-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test2-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content active" id="test2-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>Demo 3 - 没有预设值</p><div class="tabs" id="test3"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test3-1">test3 1</button></li><li class="tab"><button type="button" data-href="#test3-2">test3 2</button></li><li class="tab"><button type="button" data-href="#test3-3">test3 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test3-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>Demo 4 - 自定义Tab名 + 只有icon + icon和Tab名</p><p>MARKDOWN</p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">第一个Tab</button></li><li class="tab"><button type="button" data-href="#test4-2"><i class="fab fa-apple-pay" style="text-align: center;"></i></button></li><li class="tab"><button type="button" data-href="#test4-3"><i class="fas fa-bomb"></i>炸弹</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><p><strong>只有图标 没有Tab名字</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-3"><p><strong>名字+icon</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="段落文本"><a href="#段落文本" class="headerlink" title="段落文本"></a>段落文本</h2><ol><li><p class="p red">红色</p>2. <p class="p yellow">黄色</p>3. <p class="p green">绿色</p>4. <p class="p cyan">青色</p>5. <p class="p blue">蓝色</p>6. <p class="p gray">灰色</p>文档「开始」页面中的标题部分就是超大号文字。<p class="p center logo large">Volantis</p> <p class="p center small">A Wonderful Theme for Hexo</p>行内文本span测试彩色文字在一段话中方便插入各种颜色的标签，包括：<span class="p red">红色</span>、<span class="p yellow">黄色</span>、<span class="p green">绿色</span>、<span class="p cyan">青色</span>、<span class="p blue">蓝色</span>、<span class="p gray">灰色</span><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span> <span class="token comment">#代码测试</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>## 诗词poem测试<div class="poem"><div class="poem-title">水调歌头</div><div class="poem-author">苏轼</div><p>丙辰中秋，欢饮达旦，大醉，作此篇，兼怀子由。<br>明月几时有？把酒问青天。<br>不知天上宫阙，今夕是何年？<br>我欲乘风归去，又恐琼楼玉宇，高处不胜寒。<br>起舞弄清影，何似在人间？</p><p>转朱阁，低绮户，照无眠。<br>不应有恨，何事长向别时圆？<br>人有悲欢离合，月有阴晴圆缺，此事古难全。<br>但愿人长久，千里共婵娟。</p></div></li></ol><h2 id="行内文本样式text测试"><a href="#行内文本样式text测试" class="headerlink" title="行内文本样式text测试"></a>行内文本样式text测试</h2><ol><li>带 <u>下划线</u> 的文本</li><li>带 <emp>着重号</emp> 的文本</li><li>带 <wavy>波浪线</wavy> 的文本</li><li>带 <del>删除线</del> 的文本</li><li>键盘样式的文本 <kbd>command</kbd> + <kbd>D</kbd></li><li>密码样式的文本：<psw>这里没有验证码</psw></li></ol>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
